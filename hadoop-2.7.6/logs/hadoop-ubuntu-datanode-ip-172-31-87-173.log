2018-09-21 02:27:21,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-21 02:27:22,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-21 02:27:23,171 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-21 02:27:23,313 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-21 02:27:23,313 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-21 02:27:23,318 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-21 02:27:23,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-21 02:27:23,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-21 02:27:23,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-21 02:27:23,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-21 02:27:23,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-21 02:27:23,528 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-21 02:27:23,536 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-21 02:27:23,565 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-21 02:27:23,575 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-21 02:27:23,577 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-21 02:27:23,577 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-21 02:27:23,577 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-21 02:27:23,599 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 43760
2018-09-21 02:27:23,599 INFO org.mortbay.log: jetty-6.1.26
2018-09-21 02:27:23,823 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43760
2018-09-21 02:27:23,994 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-21 02:27:24,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-21 02:27:24,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-21 02:27:24,505 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-21 02:27:24,539 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-21 02:27:24,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-21 02:27:24,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-21 02:27:24,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-21 02:27:24,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-21 02:27:24,737 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-21 02:27:24,746 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-21 02:27:25,211 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-21 02:27:25,221 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 29308@ip-172-31-87-173.ec2.internal
2018-09-21 02:27:25,222 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-ubuntu/dfs/data is not formatted for namespace 1002891515. Formatting...
2018-09-21 02:27:25,223 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-199c2879-4a3d-4e6d-b706-569ad519cda7 for directory /tmp/hadoop-ubuntu/dfs/data
2018-09-21 02:27:25,284 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-913645330-172.31.87.173-1537496462565
2018-09-21 02:27:25,284 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /tmp/hadoop-ubuntu/dfs/data/current/BP-913645330-172.31.87.173-1537496462565
2018-09-21 02:27:25,284 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /tmp/hadoop-ubuntu/dfs/data/current/BP-913645330-172.31.87.173-1537496462565 is not formatted for BP-913645330-172.31.87.173-1537496462565. Formatting ...
2018-09-21 02:27:25,284 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-913645330-172.31.87.173-1537496462565 directory /tmp/hadoop-ubuntu/dfs/data/current/BP-913645330-172.31.87.173-1537496462565/current
2018-09-21 02:27:25,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1002891515;bpid=BP-913645330-172.31.87.173-1537496462565;lv=-56;nsInfo=lv=-63;cid=CID-83697a1b-9e98-4d1c-be74-e82cd30a8fb4;nsid=1002891515;c=0;bpid=BP-913645330-172.31.87.173-1537496462565;dnuuid=null
2018-09-21 02:27:25,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID cc2e8772-981f-47d3-a2be-07bfb5126752
2018-09-21 02:27:25,364 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-199c2879-4a3d-4e6d-b706-569ad519cda7
2018-09-21 02:27:25,364 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-ubuntu/dfs/data/current, StorageType: DISK
2018-09-21 02:27:25,380 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-09-21 02:27:25,380 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-913645330-172.31.87.173-1537496462565
2018-09-21 02:27:25,381 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-913645330-172.31.87.173-1537496462565 on volume /tmp/hadoop-ubuntu/dfs/data/current...
2018-09-21 02:27:25,405 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-913645330-172.31.87.173-1537496462565 on /tmp/hadoop-ubuntu/dfs/data/current: 24ms
2018-09-21 02:27:25,406 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-913645330-172.31.87.173-1537496462565: 25ms
2018-09-21 02:27:25,408 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-913645330-172.31.87.173-1537496462565 on volume /tmp/hadoop-ubuntu/dfs/data/current...
2018-09-21 02:27:25,408 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-913645330-172.31.87.173-1537496462565 on volume /tmp/hadoop-ubuntu/dfs/data/current: 0ms
2018-09-21 02:27:25,408 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 2ms
2018-09-21 02:27:25,410 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-913645330-172.31.87.173-1537496462565 on volume /tmp/hadoop-ubuntu/dfs/data
2018-09-21 02:27:25,410 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-ubuntu/dfs/data, DS-199c2879-4a3d-4e6d-b706-569ad519cda7): finished scanning block pool BP-913645330-172.31.87.173-1537496462565
2018-09-21 02:27:25,416 ERROR org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: dfs.datanode.directoryscan.throttle.limit.ms.per.sec set to value below 1 ms/sec. Assuming default value of 1000
2018-09-21 02:27:25,416 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1537511030416ms with interval of 21600000ms
2018-09-21 02:27:25,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-913645330-172.31.87.173-1537496462565 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2018-09-21 02:27:25,520 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-ubuntu/dfs/data, DS-199c2879-4a3d-4e6d-b706-569ad519cda7): no suitable block pools found to scan.  Waiting 1814399890 ms.
2018-09-21 02:27:25,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-913645330-172.31.87.173-1537496462565 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2018-09-21 02:27:25,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-09-21 02:27:25,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-913645330-172.31.87.173-1537496462565 (Datanode Uuid cc2e8772-981f-47d3-a2be-07bfb5126752) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2018-09-21 02:27:25,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-913645330-172.31.87.173-1537496462565 (Datanode Uuid cc2e8772-981f-47d3-a2be-07bfb5126752) service to localhost/127.0.0.1:9000
2018-09-21 02:27:25,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x223b19e609887,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 2 msec to generate and 164 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-21 02:27:25,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-913645330-172.31.87.173-1537496462565
2018-09-21 02:41:53,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741825_1001 src: /127.0.0.1:46402 dest: /127.0.0.1:50010
2018-09-21 02:41:53,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46402, dest: /127.0.0.1:50010, bytes: 4436, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741825_1001, duration: 119687294
2018-09-21 02:41:53,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:53,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741826_1002 src: /127.0.0.1:46404 dest: /127.0.0.1:50010
2018-09-21 02:41:53,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46404, dest: /127.0.0.1:50010, bytes: 1335, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741826_1002, duration: 853039
2018-09-21 02:41:53,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:53,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741827_1003 src: /127.0.0.1:46406 dest: /127.0.0.1:50010
2018-09-21 02:41:53,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46406, dest: /127.0.0.1:50010, bytes: 318, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741827_1003, duration: 1630980
2018-09-21 02:41:53,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:53,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741828_1004 src: /127.0.0.1:46408 dest: /127.0.0.1:50010
2018-09-21 02:41:53,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46408, dest: /127.0.0.1:50010, bytes: 884, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741828_1004, duration: 881856
2018-09-21 02:41:53,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:53,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741829_1005 src: /127.0.0.1:46410 dest: /127.0.0.1:50010
2018-09-21 02:41:53,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46410, dest: /127.0.0.1:50010, bytes: 3670, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741829_1005, duration: 837488
2018-09-21 02:41:53,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741830_1006 src: /127.0.0.1:46412 dest: /127.0.0.1:50010
2018-09-21 02:41:54,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46412, dest: /127.0.0.1:50010, bytes: 4239, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741830_1006, duration: 1936040
2018-09-21 02:41:54,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741831_1007 src: /127.0.0.1:46414 dest: /127.0.0.1:50010
2018-09-21 02:41:54,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46414, dest: /127.0.0.1:50010, bytes: 2490, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741831_1007, duration: 1317914
2018-09-21 02:41:54,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741832_1008 src: /127.0.0.1:46416 dest: /127.0.0.1:50010
2018-09-21 02:41:54,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46416, dest: /127.0.0.1:50010, bytes: 2598, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741832_1008, duration: 1628645
2018-09-21 02:41:54,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741833_1009 src: /127.0.0.1:46418 dest: /127.0.0.1:50010
2018-09-21 02:41:54,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46418, dest: /127.0.0.1:50010, bytes: 9683, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741833_1009, duration: 2211031
2018-09-21 02:41:54,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741834_1010 src: /127.0.0.1:46420 dest: /127.0.0.1:50010
2018-09-21 02:41:54,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46420, dest: /127.0.0.1:50010, bytes: 867, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741834_1010, duration: 3726335
2018-09-21 02:41:54,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741835_1011 src: /127.0.0.1:46422 dest: /127.0.0.1:50010
2018-09-21 02:41:54,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46422, dest: /127.0.0.1:50010, bytes: 1449, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741835_1011, duration: 955876
2018-09-21 02:41:54,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741836_1012 src: /127.0.0.1:46424 dest: /127.0.0.1:50010
2018-09-21 02:41:54,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46424, dest: /127.0.0.1:50010, bytes: 1657, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741836_1012, duration: 779423
2018-09-21 02:41:54,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741837_1013 src: /127.0.0.1:46426 dest: /127.0.0.1:50010
2018-09-21 02:41:54,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46426, dest: /127.0.0.1:50010, bytes: 21, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741837_1013, duration: 844699
2018-09-21 02:41:54,259 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741838_1014 src: /127.0.0.1:46428 dest: /127.0.0.1:50010
2018-09-21 02:41:54,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46428, dest: /127.0.0.1:50010, bytes: 620, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741838_1014, duration: 1422832
2018-09-21 02:41:54,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741839_1015 src: /127.0.0.1:46430 dest: /127.0.0.1:50010
2018-09-21 02:41:54,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46430, dest: /127.0.0.1:50010, bytes: 3518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741839_1015, duration: 807478
2018-09-21 02:41:54,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741840_1016 src: /127.0.0.1:46432 dest: /127.0.0.1:50010
2018-09-21 02:41:54,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46432, dest: /127.0.0.1:50010, bytes: 1527, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741840_1016, duration: 1371210
2018-09-21 02:41:54,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741841_1017 src: /127.0.0.1:46434 dest: /127.0.0.1:50010
2018-09-21 02:41:54,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46434, dest: /127.0.0.1:50010, bytes: 1631, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741841_1017, duration: 1013142
2018-09-21 02:41:54,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741842_1018 src: /127.0.0.1:46436 dest: /127.0.0.1:50010
2018-09-21 02:41:54,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46436, dest: /127.0.0.1:50010, bytes: 5540, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741842_1018, duration: 2216554
2018-09-21 02:41:54,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741842_1018, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741843_1019 src: /127.0.0.1:46438 dest: /127.0.0.1:50010
2018-09-21 02:41:54,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46438, dest: /127.0.0.1:50010, bytes: 11801, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741843_1019, duration: 787240
2018-09-21 02:41:54,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741843_1019, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741844_1020 src: /127.0.0.1:46440 dest: /127.0.0.1:50010
2018-09-21 02:41:54,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46440, dest: /127.0.0.1:50010, bytes: 951, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741844_1020, duration: 1087769
2018-09-21 02:41:54,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741844_1020, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741845_1021 src: /127.0.0.1:46442 dest: /127.0.0.1:50010
2018-09-21 02:41:54,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46442, dest: /127.0.0.1:50010, bytes: 1383, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741845_1021, duration: 840473
2018-09-21 02:41:54,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741845_1021, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741846_1022 src: /127.0.0.1:46444 dest: /127.0.0.1:50010
2018-09-21 02:41:54,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46444, dest: /127.0.0.1:50010, bytes: 4113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741846_1022, duration: 1512051
2018-09-21 02:41:54,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741846_1022, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741847_1023 src: /127.0.0.1:46446 dest: /127.0.0.1:50010
2018-09-21 02:41:54,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46446, dest: /127.0.0.1:50010, bytes: 758, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741847_1023, duration: 4451449
2018-09-21 02:41:54,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741847_1023, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741848_1024 src: /127.0.0.1:46448 dest: /127.0.0.1:50010
2018-09-21 02:41:54,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46448, dest: /127.0.0.1:50010, bytes: 10, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741848_1024, duration: 2675723
2018-09-21 02:41:54,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741848_1024, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741849_1025 src: /127.0.0.1:46450 dest: /127.0.0.1:50010
2018-09-21 02:41:54,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46450, dest: /127.0.0.1:50010, bytes: 2316, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741849_1025, duration: 568862
2018-09-21 02:41:54,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741849_1025, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741850_1026 src: /127.0.0.1:46452 dest: /127.0.0.1:50010
2018-09-21 02:41:54,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46452, dest: /127.0.0.1:50010, bytes: 2697, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741850_1026, duration: 560066
2018-09-21 02:41:54,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741851_1027 src: /127.0.0.1:46454 dest: /127.0.0.1:50010
2018-09-21 02:41:54,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46454, dest: /127.0.0.1:50010, bytes: 2191, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741851_1027, duration: 532220
2018-09-21 02:41:54,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741851_1027, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741852_1028 src: /127.0.0.1:46456 dest: /127.0.0.1:50010
2018-09-21 02:41:54,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46456, dest: /127.0.0.1:50010, bytes: 4567, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741852_1028, duration: 550002
2018-09-21 02:41:54,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:41:54,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741853_1029 src: /127.0.0.1:46458 dest: /127.0.0.1:50010
2018-09-21 02:41:54,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46458, dest: /127.0.0.1:50010, bytes: 690, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957136671_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741853_1029, duration: 868685
2018-09-21 02:41:54,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741853_1029, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:23,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741854_1030 src: /127.0.0.1:46462 dest: /127.0.0.1:50010
2018-09-21 02:42:23,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46462, dest: /127.0.0.1:50010, bytes: 4436, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741854_1030, duration: 60528470
2018-09-21 02:42:23,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741854_1030, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:23,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741855_1031 src: /127.0.0.1:46464 dest: /127.0.0.1:50010
2018-09-21 02:42:23,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46464, dest: /127.0.0.1:50010, bytes: 1335, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741855_1031, duration: 590562
2018-09-21 02:42:23,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741855_1031, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:23,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741856_1032 src: /127.0.0.1:46466 dest: /127.0.0.1:50010
2018-09-21 02:42:23,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46466, dest: /127.0.0.1:50010, bytes: 318, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741856_1032, duration: 1023861
2018-09-21 02:42:23,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741856_1032, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:23,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741857_1033 src: /127.0.0.1:46468 dest: /127.0.0.1:50010
2018-09-21 02:42:23,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46468, dest: /127.0.0.1:50010, bytes: 884, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741857_1033, duration: 746503
2018-09-21 02:42:23,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741857_1033, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:23,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741858_1034 src: /127.0.0.1:46470 dest: /127.0.0.1:50010
2018-09-21 02:42:23,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46470, dest: /127.0.0.1:50010, bytes: 3670, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741858_1034, duration: 1092086
2018-09-21 02:42:23,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741858_1034, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:23,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741859_1035 src: /127.0.0.1:46472 dest: /127.0.0.1:50010
2018-09-21 02:42:23,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46472, dest: /127.0.0.1:50010, bytes: 4239, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741859_1035, duration: 1640344
2018-09-21 02:42:23,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741859_1035, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:23,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741860_1036 src: /127.0.0.1:46474 dest: /127.0.0.1:50010
2018-09-21 02:42:23,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46474, dest: /127.0.0.1:50010, bytes: 2490, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741860_1036, duration: 526148
2018-09-21 02:42:23,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741860_1036, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:23,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741861_1037 src: /127.0.0.1:46476 dest: /127.0.0.1:50010
2018-09-21 02:42:23,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46476, dest: /127.0.0.1:50010, bytes: 2598, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741861_1037, duration: 863968
2018-09-21 02:42:23,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741861_1037, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:23,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741862_1038 src: /127.0.0.1:46478 dest: /127.0.0.1:50010
2018-09-21 02:42:23,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46478, dest: /127.0.0.1:50010, bytes: 9683, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741862_1038, duration: 823594
2018-09-21 02:42:23,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741862_1038, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741863_1039 src: /127.0.0.1:46480 dest: /127.0.0.1:50010
2018-09-21 02:42:24,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46480, dest: /127.0.0.1:50010, bytes: 867, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741863_1039, duration: 6282112
2018-09-21 02:42:24,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741863_1039, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741864_1040 src: /127.0.0.1:46482 dest: /127.0.0.1:50010
2018-09-21 02:42:24,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46482, dest: /127.0.0.1:50010, bytes: 1449, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741864_1040, duration: 543386
2018-09-21 02:42:24,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741864_1040, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741865_1041 src: /127.0.0.1:46484 dest: /127.0.0.1:50010
2018-09-21 02:42:24,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46484, dest: /127.0.0.1:50010, bytes: 1657, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741865_1041, duration: 873235
2018-09-21 02:42:24,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741865_1041, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741866_1042 src: /127.0.0.1:46486 dest: /127.0.0.1:50010
2018-09-21 02:42:24,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46486, dest: /127.0.0.1:50010, bytes: 21, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741866_1042, duration: 454973
2018-09-21 02:42:24,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741866_1042, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741867_1043 src: /127.0.0.1:46488 dest: /127.0.0.1:50010
2018-09-21 02:42:24,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46488, dest: /127.0.0.1:50010, bytes: 620, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741867_1043, duration: 1627816
2018-09-21 02:42:24,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741867_1043, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741868_1044 src: /127.0.0.1:46490 dest: /127.0.0.1:50010
2018-09-21 02:42:24,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46490, dest: /127.0.0.1:50010, bytes: 3518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741868_1044, duration: 494744
2018-09-21 02:42:24,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741868_1044, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741869_1045 src: /127.0.0.1:46492 dest: /127.0.0.1:50010
2018-09-21 02:42:24,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46492, dest: /127.0.0.1:50010, bytes: 1527, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741869_1045, duration: 668928
2018-09-21 02:42:24,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741869_1045, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741870_1046 src: /127.0.0.1:46494 dest: /127.0.0.1:50010
2018-09-21 02:42:24,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46494, dest: /127.0.0.1:50010, bytes: 1631, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741870_1046, duration: 478291
2018-09-21 02:42:24,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741870_1046, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741871_1047 src: /127.0.0.1:46496 dest: /127.0.0.1:50010
2018-09-21 02:42:24,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46496, dest: /127.0.0.1:50010, bytes: 5540, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741871_1047, duration: 558888
2018-09-21 02:42:24,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741871_1047, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741872_1048 src: /127.0.0.1:46498 dest: /127.0.0.1:50010
2018-09-21 02:42:24,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46498, dest: /127.0.0.1:50010, bytes: 11801, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741872_1048, duration: 490032
2018-09-21 02:42:24,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741872_1048, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741873_1049 src: /127.0.0.1:46500 dest: /127.0.0.1:50010
2018-09-21 02:42:24,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46500, dest: /127.0.0.1:50010, bytes: 951, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741873_1049, duration: 3604038
2018-09-21 02:42:24,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741873_1049, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741874_1050 src: /127.0.0.1:46502 dest: /127.0.0.1:50010
2018-09-21 02:42:24,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46502, dest: /127.0.0.1:50010, bytes: 1383, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741874_1050, duration: 790701
2018-09-21 02:42:24,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741874_1050, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741875_1051 src: /127.0.0.1:46504 dest: /127.0.0.1:50010
2018-09-21 02:42:24,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46504, dest: /127.0.0.1:50010, bytes: 4113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741875_1051, duration: 4262728
2018-09-21 02:42:24,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741875_1051, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741876_1052 src: /127.0.0.1:46506 dest: /127.0.0.1:50010
2018-09-21 02:42:24,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46506, dest: /127.0.0.1:50010, bytes: 758, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741876_1052, duration: 2544610
2018-09-21 02:42:24,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741876_1052, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741877_1053 src: /127.0.0.1:46508 dest: /127.0.0.1:50010
2018-09-21 02:42:24,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46508, dest: /127.0.0.1:50010, bytes: 10, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741877_1053, duration: 1846008
2018-09-21 02:42:24,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741877_1053, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741878_1054 src: /127.0.0.1:46510 dest: /127.0.0.1:50010
2018-09-21 02:42:24,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46510, dest: /127.0.0.1:50010, bytes: 2316, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741878_1054, duration: 596834
2018-09-21 02:42:24,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741878_1054, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741879_1055 src: /127.0.0.1:46512 dest: /127.0.0.1:50010
2018-09-21 02:42:24,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46512, dest: /127.0.0.1:50010, bytes: 2697, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741879_1055, duration: 1296024
2018-09-21 02:42:24,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741879_1055, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741880_1056 src: /127.0.0.1:46514 dest: /127.0.0.1:50010
2018-09-21 02:42:24,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46514, dest: /127.0.0.1:50010, bytes: 2191, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741880_1056, duration: 1856592
2018-09-21 02:42:24,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741880_1056, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741881_1057 src: /127.0.0.1:46516 dest: /127.0.0.1:50010
2018-09-21 02:42:24,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46516, dest: /127.0.0.1:50010, bytes: 4567, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741881_1057, duration: 550020
2018-09-21 02:42:24,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741881_1057, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 02:42:24,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741882_1058 src: /127.0.0.1:46518 dest: /127.0.0.1:50010
2018-09-21 02:42:24,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46518, dest: /127.0.0.1:50010, bytes: 690, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1004031620_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741882_1058, duration: 4214907
2018-09-21 02:42:24,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741882_1058, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 03:02:38,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741883_1059 src: /127.0.0.1:46574 dest: /127.0.0.1:50010
2018-09-21 03:02:38,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46574, dest: /127.0.0.1:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_3872724_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741883_1059, duration: 65596218
2018-09-21 03:02:38,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741883_1059, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 03:02:40,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741884_1060 src: /127.0.0.1:46578 dest: /127.0.0.1:50010
2018-09-21 03:02:40,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46578, dest: /127.0.0.1:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_3872724_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741884_1060, duration: 982423
2018-09-21 03:02:40,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741884_1060, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 03:02:40,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741885_1061 src: /127.0.0.1:46580 dest: /127.0.0.1:50010
2018-09-21 03:02:40,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46580, dest: /127.0.0.1:50010, bytes: 97, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_3872724_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741885_1061, duration: 1677731
2018-09-21 03:02:40,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741885_1061, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-21 03:02:43,658 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741883_1059 file /tmp/hadoop-ubuntu/dfs/data/current/BP-913645330-172.31.87.173-1537496462565/current/finalized/subdir0/subdir0/blk_1073741883 for deletion
2018-09-21 03:02:43,660 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741884_1060 file /tmp/hadoop-ubuntu/dfs/data/current/BP-913645330-172.31.87.173-1537496462565/current/finalized/subdir0/subdir0/blk_1073741884 for deletion
2018-09-21 03:02:43,660 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741885_1061 file /tmp/hadoop-ubuntu/dfs/data/current/BP-913645330-172.31.87.173-1537496462565/current/finalized/subdir0/subdir0/blk_1073741885 for deletion
2018-09-21 03:02:43,661 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-913645330-172.31.87.173-1537496462565 blk_1073741883_1059 file /tmp/hadoop-ubuntu/dfs/data/current/BP-913645330-172.31.87.173-1537496462565/current/finalized/subdir0/subdir0/blk_1073741883
2018-09-21 03:02:43,661 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-913645330-172.31.87.173-1537496462565 blk_1073741884_1060 file /tmp/hadoop-ubuntu/dfs/data/current/BP-913645330-172.31.87.173-1537496462565/current/finalized/subdir0/subdir0/blk_1073741884
2018-09-21 03:02:43,662 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-913645330-172.31.87.173-1537496462565 blk_1073741885_1061 file /tmp/hadoop-ubuntu/dfs/data/current/BP-913645330-172.31.87.173-1537496462565/current/finalized/subdir0/subdir0/blk_1073741885
2018-09-21 06:23:50,444 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-913645330-172.31.87.173-1537496462565 Total blocks: 58, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-21 07:30:29,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2343b59abb219,  containing 1 storage report(s), of which we sent 1. The reports had 58 total blocks and used 1 RPC(s). This took 4 msec to generate and 8 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-21 07:30:29,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-913645330-172.31.87.173-1537496462565
2018-09-21 12:23:50,428 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-913645330-172.31.87.173-1537496462565 Total blocks: 58, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-21 13:30:27,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x247e011e39e9c,  containing 1 storage report(s), of which we sent 1. The reports had 58 total blocks and used 1 RPC(s). This took 3 msec to generate and 6 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-21 13:30:27,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-913645330-172.31.87.173-1537496462565
2018-09-21 18:23:50,424 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-913645330-172.31.87.173-1537496462565 Total blocks: 58, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-21 19:30:28,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x25b857c9fa02e,  containing 1 storage report(s), of which we sent 1. The reports had 58 total blocks and used 1 RPC(s). This took 1 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-21 19:30:28,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-913645330-172.31.87.173-1537496462565
2018-09-22 00:23:50,426 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-913645330-172.31.87.173-1537496462565 Total blocks: 58, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-22 01:30:27,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x26f2a38152c17,  containing 1 storage report(s), of which we sent 1. The reports had 58 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-22 01:30:27,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-913645330-172.31.87.173-1537496462565
2018-09-22 06:23:50,422 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-913645330-172.31.87.173-1537496462565 Total blocks: 58, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-22 07:30:28,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x282cfa0deedc8,  containing 1 storage report(s), of which we sent 1. The reports had 58 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-22 07:30:28,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-913645330-172.31.87.173-1537496462565
2018-09-22 12:23:50,421 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-913645330-172.31.87.173-1537496462565 Total blocks: 58, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-22 13:30:29,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x296750cb9e4c5,  containing 1 storage report(s), of which we sent 1. The reports had 58 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-22 13:30:29,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-913645330-172.31.87.173-1537496462565
2018-09-22 18:23:50,424 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-913645330-172.31.87.173-1537496462565 Total blocks: 58, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-22 19:30:27,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2aa19c485606d,  containing 1 storage report(s), of which we sent 1. The reports had 58 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-22 19:30:27,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-913645330-172.31.87.173-1537496462565
2018-09-23 00:23:50,422 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-913645330-172.31.87.173-1537496462565 Total blocks: 58, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-23 01:30:28,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2bdbf31917f80,  containing 1 storage report(s), of which we sent 1. The reports had 58 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-23 01:30:28,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-913645330-172.31.87.173-1537496462565
2018-09-23 06:23:50,422 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-913645330-172.31.87.173-1537496462565 Total blocks: 58, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-23 07:18:06,000 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ip-172-31-87-173.ec2.internal/172.31.87.173"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1413)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:152)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:402)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:659)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1085)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:980)
2018-09-23 07:18:09,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 07:18:10,497 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-09-23 07:18:10,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 07:21:09,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 07:21:09,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 07:21:10,685 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 07:21:10,805 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 07:21:10,805 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 07:21:10,811 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 07:21:10,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 07:21:10,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 07:21:10,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 07:21:10,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 07:21:10,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 07:21:11,009 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 07:21:11,023 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 07:21:11,054 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 07:21:11,066 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 07:21:11,068 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 07:21:11,068 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 07:21:11,069 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 07:21:11,087 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 38602
2018-09-23 07:21:11,087 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 07:21:11,319 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:38602
2018-09-23 07:21:11,483 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 07:21:11,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 07:21:11,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 07:21:12,002 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 07:21:12,035 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 07:21:12,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 07:21:12,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 07:21:12,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 07:21:12,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 07:21:12,205 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 07:21:12,208 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 07:21:12,700 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 07:21:12,707 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 12880@ip-172-31-87-173.ec2.internal
2018-09-23 07:21:12,792 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-913645330-172.31.87.173-1537496462565
2018-09-23 07:21:12,792 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /tmp/hadoop-ubuntu/dfs/data/current/BP-913645330-172.31.87.173-1537496462565
2018-09-23 07:21:12,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1002891515;bpid=BP-913645330-172.31.87.173-1537496462565;lv=-56;nsInfo=lv=-63;cid=CID-83697a1b-9e98-4d1c-be74-e82cd30a8fb4;nsid=1002891515;c=0;bpid=BP-913645330-172.31.87.173-1537496462565;dnuuid=cc2e8772-981f-47d3-a2be-07bfb5126752
2018-09-23 07:21:12,870 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-199c2879-4a3d-4e6d-b706-569ad519cda7
2018-09-23 07:21:12,870 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-ubuntu/dfs/data/current, StorageType: DISK
2018-09-23 07:21:12,927 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-09-23 07:21:12,928 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-913645330-172.31.87.173-1537496462565
2018-09-23 07:21:12,929 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-913645330-172.31.87.173-1537496462565 on volume /tmp/hadoop-ubuntu/dfs/data/current...
2018-09-23 07:21:12,946 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /tmp/hadoop-ubuntu/dfs/data/current/BP-913645330-172.31.87.173-1537496462565/current: 585728
2018-09-23 07:21:12,951 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-913645330-172.31.87.173-1537496462565 on /tmp/hadoop-ubuntu/dfs/data/current: 22ms
2018-09-23 07:21:12,951 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-913645330-172.31.87.173-1537496462565: 23ms
2018-09-23 07:21:12,954 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-913645330-172.31.87.173-1537496462565 on volume /tmp/hadoop-ubuntu/dfs/data/current...
2018-09-23 07:21:12,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-913645330-172.31.87.173-1537496462565 on volume /tmp/hadoop-ubuntu/dfs/data/current: 26ms
2018-09-23 07:21:12,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 29ms
2018-09-23 07:21:13,074 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-ubuntu/dfs/data, DS-199c2879-4a3d-4e6d-b706-569ad519cda7): no suitable block pools found to scan.  Waiting 1623972336 ms.
2018-09-23 07:21:13,076 ERROR org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: dfs.datanode.directoryscan.throttle.limit.ms.per.sec set to value below 1 ms/sec. Assuming default value of 1000
2018-09-23 07:21:13,076 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1537688257076ms with interval of 21600000ms
2018-09-23 07:21:13,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-913645330-172.31.87.173-1537496462565 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2018-09-23 07:21:13,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-913645330-172.31.87.173-1537496462565 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2018-09-23 07:21:13,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-09-23 07:21:13,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-913645330-172.31.87.173-1537496462565 (Datanode Uuid cc2e8772-981f-47d3-a2be-07bfb5126752) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=506
2018-09-23 07:21:13,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-913645330-172.31.87.173-1537496462565 (Datanode Uuid cc2e8772-981f-47d3-a2be-07bfb5126752) service to localhost/127.0.0.1:9000
2018-09-23 07:21:13,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2d0e305501f07,  containing 1 storage report(s), of which we sent 1. The reports had 58 total blocks and used 1 RPC(s). This took 12 msec to generate and 180 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-23 07:21:13,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-913645330-172.31.87.173-1537496462565
2018-09-23 07:37:37,101 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-913645330-172.31.87.173-1537496462565 Total blocks: 58, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-23 08:17:59,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741886_1062 src: /127.0.0.1:57294 dest: /127.0.0.1:50010
2018-09-23 08:17:59,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57294, dest: /127.0.0.1:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_146258501_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741886_1062, duration: 111881201
2018-09-23 08:17:59,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741886_1062, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 08:22:05,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741887_1063 src: /127.0.0.1:57330 dest: /127.0.0.1:50010
2018-09-23 08:22:05,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57330, dest: /127.0.0.1:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_980167818_1, offset: 0, srvID: cc2e8772-981f-47d3-a2be-07bfb5126752, blockid: BP-913645330-172.31.87.173-1537496462565:blk_1073741887_1063, duration: 55734862
2018-09-23 08:22:05,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-913645330-172.31.87.173-1537496462565:blk_1073741887_1063, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 08:26:11,556 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1365ms
No GCs detected
2018-09-23 08:29:00,592 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2769ms
No GCs detected
2018-09-23 08:30:27,056 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2573ms
No GCs detected
2018-09-23 08:31:27,340 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2010ms
No GCs detected
2018-09-23 08:32:11,862 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3235ms
No GCs detected
2018-09-23 08:32:41,716 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 14166ms
GC pool 'Copy' had collection(s): count=1 time=12358ms
2018-09-23 08:33:00,129 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1012ms
No GCs detected
2018-09-23 08:33:04,594 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1046ms
No GCs detected
2018-09-23 08:33:08,410 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1774ms
No GCs detected
2018-09-23 08:33:14,227 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1774ms
No GCs detected
2018-09-23 08:33:27,154 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1082ms
No GCs detected
2018-09-23 08:33:55,168 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5424ms
No GCs detected
2018-09-23 08:34:55,636 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 59051ms
No GCs detected
2018-09-23 08:35:03,646 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3526ms
No GCs detected
2018-09-23 08:35:15,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741888_1064 src: /127.0.0.1:57382 dest: /127.0.0.1:50010
2018-09-23 08:35:15,256 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4699ms
No GCs detected
2018-09-23 08:35:21,184 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1242ms
No GCs detected
2018-09-23 08:35:45,372 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2104ms
No GCs detected
2018-09-23 08:36:06,093 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2506ms
No GCs detected
2018-09-23 08:36:09,667 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1012ms
No GCs detected
2018-09-23 08:36:13,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-913645330-172.31.87.173-1537496462565:blk_1073741889_1065 src: /127.0.0.1:57400 dest: /127.0.0.1:50010
2018-09-23 08:36:26,679 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4317ms
No GCs detected
2018-09-23 08:36:37,086 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2255ms
No GCs detected
2018-09-23 08:36:45,903 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 7884ms
No GCs detected
2018-09-23 08:37:02,062 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5025ms
No GCs detected
2018-09-23 08:37:11,046 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1676ms
No GCs detected
2018-09-23 08:37:25,311 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4851ms
No GCs detected
2018-09-23 08:37:30,740 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2198ms
No GCs detected
2018-09-23 08:38:08,569 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1267ms
No GCs detected
2018-09-23 08:38:13,753 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1341ms
No GCs detected
2018-09-23 08:38:22,802 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1135ms
No GCs detected
2018-09-23 08:38:29,497 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2774ms
No GCs detected
2018-09-23 08:38:36,018 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2497ms
No GCs detected
2018-09-23 08:38:45,626 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2338ms
No GCs detected
2018-09-23 08:38:55,575 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3315ms
No GCs detected
2018-09-23 08:38:59,074 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2578ms
No GCs detected
2018-09-23 08:39:11,715 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2968ms
No GCs detected
2018-09-23 08:39:15,182 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2565ms
No GCs detected
2018-09-23 08:39:20,180 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4188ms
No GCs detected
2018-09-23 08:39:22,581 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1395ms
No GCs detected
2018-09-23 08:39:27,137 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1124ms
No GCs detected
2018-09-23 08:39:35,313 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2827ms
No GCs detected
2018-09-23 08:39:41,043 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1578ms
No GCs detected
2018-09-23 08:39:47,181 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-09-23 08:39:47,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 08:44:11,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 08:44:11,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 08:44:12,923 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 08:44:13,044 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 08:44:13,044 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 08:44:13,049 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 08:44:13,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 08:44:13,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 08:44:13,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 08:44:13,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 08:44:13,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 08:44:13,254 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 08:44:13,266 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 08:44:13,294 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 08:44:13,306 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 08:44:13,308 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 08:44:13,308 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 08:44:13,308 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 08:44:13,331 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 44787
2018-09-23 08:44:13,332 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 08:44:13,589 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:44787
2018-09-23 08:44:13,757 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 08:44:14,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 08:44:14,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 08:44:14,267 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 08:44:14,302 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 08:44:14,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 08:44:14,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 08:44:14,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 08:44:14,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 08:44:14,509 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 08:44:14,513 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 08:44:15,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:16,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:17,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:18,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:19,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:20,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:21,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:22,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:23,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:24,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:24,697 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:44:30,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:31,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:32,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:33,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:34,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:35,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:36,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:37,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:38,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:39,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:39,713 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:44:45,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:46,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:47,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:48,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:49,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:50,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:51,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:52,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:53,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:54,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:44:54,724 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:45:00,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:01,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:02,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:03,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:04,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:05,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:06,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:07,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:08,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:09,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:09,736 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:45:15,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:16,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:17,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:18,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:19,740 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:20,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:21,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:22,743 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:23,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:24,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:24,747 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:45:30,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:31,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:32,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:33,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:34,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:35,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:36,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:37,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:38,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:39,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:39,757 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:45:45,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:46,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:47,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:48,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:49,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:50,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:51,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:52,769 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:53,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:54,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:45:54,774 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:46:00,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:01,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:02,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:03,777 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:04,777 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:05,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:06,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:07,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:08,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:09,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:09,785 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:46:15,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:16,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:17,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:18,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:19,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:20,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:21,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:22,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:23,793 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:24,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:24,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:46:30,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:31,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:32,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:33,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:34,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:35,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:36,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:37,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:38,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:39,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:39,806 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:46:45,808 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:46,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:47,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:48,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:49,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:50,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:51,815 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:52,815 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:53,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:54,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:46:54,821 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:47:00,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:01,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:02,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:03,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:04,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:05,828 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:06,828 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:07,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:08,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:09,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:09,833 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:47:15,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:16,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:17,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:18,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:19,837 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:20,839 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:21,839 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:22,840 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:23,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:24,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:24,845 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:47:30,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:31,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:32,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:33,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:34,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:35,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:36,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:37,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:38,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:39,851 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:39,853 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:47:45,854 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:46,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:47,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:48,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:49,857 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:50,858 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:51,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:52,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:53,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:54,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:47:54,862 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:48:00,865 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:01,865 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:02,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:03,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:04,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:05,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:06,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:07,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:08,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:09,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:09,877 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:48:15,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:16,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:17,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:18,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:19,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:20,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:21,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:22,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:23,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:24,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:24,887 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:48:30,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:31,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:32,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:33,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:34,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:35,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:36,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:37,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:38,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:39,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:39,899 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:48:45,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:46,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:47,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:48,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:49,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:50,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:51,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:52,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:53,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:54,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:48:54,912 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:49:00,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:01,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:02,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:03,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:04,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:05,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:06,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:07,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:08,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:09,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:09,926 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:49:15,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:16,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:17,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:18,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:19,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:20,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:21,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:22,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:23,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:24,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:24,941 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:49:30,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:31,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:32,944 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:33,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:34,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:35,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:36,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:37,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:38,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:39,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:39,952 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:49:45,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:46,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:47,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:48,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:49,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:50,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:51,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:52,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:53,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:54,965 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:49:54,970 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:50:00,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:01,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:02,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:03,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:04,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:05,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:06,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:07,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:08,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:09,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:09,978 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:50:15,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:16,980 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:17,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:18,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:19,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:20,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:21,985 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:22,985 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:23,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:24,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:24,990 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:50:30,992 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:31,992 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:32,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:33,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:34,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:35,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:36,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:37,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:38,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:39,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:39,999 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:50:46,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:47,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:48,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:49,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:50,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:51,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:52,005 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:53,005 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:54,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:55,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:50:55,009 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:51:01,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:02,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:03,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:04,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:05,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:06,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:07,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:08,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:09,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:10,016 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:10,019 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:51:16,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:17,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:18,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:19,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:20,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:21,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:22,025 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:23,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:24,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:25,027 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:25,029 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:51:31,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:32,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:33,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:34,032 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:35,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:36,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:37,034 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:38,034 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:39,035 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:40,035 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:40,044 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:51:46,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:47,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:48,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:49,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:50,048 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:51,050 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:52,050 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:53,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:54,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:55,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:51:55,054 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:52:01,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:02,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:03,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:04,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:05,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:06,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:07,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:08,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:09,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:10,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:10,069 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:52:16,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:17,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:18,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:19,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:20,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:21,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:22,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:23,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:24,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:25,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:25,087 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:52:31,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:32,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:33,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:34,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:35,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:36,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:37,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:38,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:39,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:40,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:40,095 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:52:46,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:47,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:48,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:49,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:50,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:51,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:52,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:53,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:54,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:55,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:52:55,105 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:53:01,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:02,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:03,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:04,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:05,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:06,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:07,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:08,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:09,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:10,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:10,114 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:53:16,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:17,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:18,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:19,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:20,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:21,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:22,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:23,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:24,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:25,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:25,124 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:53:31,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:32,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:33,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:34,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:35,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:36,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:37,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:38,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:39,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:40,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:40,135 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:53:46,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:47,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:48,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:49,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:50,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:51,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:52,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:53,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:54,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:55,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:53:55,148 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:54:01,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:02,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:03,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:04,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:05,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:06,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:07,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:08,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:09,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:10,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:10,163 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:54:16,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:17,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:18,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:19,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:20,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:21,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:22,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:23,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:24,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:25,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:25,173 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:54:31,175 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:32,176 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:33,176 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:34,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:35,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:36,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:37,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:38,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:39,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:40,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:40,183 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:54:46,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:47,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:48,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:54:48,570 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-09-23 08:54:48,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 08:55:27,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 08:55:27,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 08:55:29,036 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 08:55:29,188 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 08:55:29,189 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 08:55:29,197 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 08:55:29,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 08:55:29,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 08:55:29,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 08:55:29,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 08:55:29,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 08:55:29,403 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 08:55:29,416 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 08:55:29,445 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 08:55:29,457 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 08:55:29,459 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 08:55:29,459 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 08:55:29,459 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 08:55:29,478 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 44273
2018-09-23 08:55:29,478 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 08:55:29,697 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:44273
2018-09-23 08:55:29,836 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 08:55:30,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 08:55:30,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 08:55:30,333 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 08:55:30,362 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 08:55:30,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 08:55:30,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 08:55:30,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 08:55:30,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 08:55:30,552 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 08:55:30,554 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 08:55:31,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:32,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:33,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:34,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:35,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:36,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:37,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:38,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:39,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:40,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:40,700 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:55:46,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:47,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:48,703 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:49,703 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:50,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:51,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:52,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:53,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:54,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:55,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:55:55,711 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:56:01,713 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:02,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:03,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:04,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:05,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:06,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:07,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:08,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:09,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:10,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:10,722 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:56:16,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:17,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:18,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:19,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:20,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:21,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:22,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:23,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:24,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:25,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:25,736 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:56:31,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:32,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:33,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:34,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:35,740 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:36,743 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:37,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:38,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:39,745 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:40,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:40,748 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:56:46,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:47,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:48,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:49,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:50,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:51,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:52,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:53,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:54,756 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:55,757 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:56:55,759 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:57:01,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:02,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:03,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:04,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:05,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:06,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:07,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:08,765 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:09,766 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:10,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:10,770 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:57:16,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:17,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:18,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:19,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:20,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:21,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:22,777 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:23,777 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:24,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:25,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:25,782 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:57:31,783 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:32,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:33,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:34,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:35,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:36,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:37,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:38,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:39,790 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:40,790 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:40,796 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:57:46,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:47,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:48,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:49,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:50,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:51,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:52,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:53,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:54,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:55,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:57:55,805 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:58:01,806 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:02,807 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:03,808 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:04,808 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:05,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:06,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:07,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:08,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:09,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:10,814 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:10,817 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:58:16,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:17,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:18,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:19,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:20,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:21,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:22,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:23,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:24,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:25,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:25,829 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:58:31,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:32,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:33,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:34,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:35,833 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:36,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:37,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:38,837 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:39,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:40,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:40,842 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:58:46,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:47,844 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:48,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:49,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:50,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:51,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:52,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:53,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:54,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:55,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:58:55,851 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:59:01,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:02,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:03,853 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:04,854 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:05,854 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:06,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:07,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:08,857 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:09,858 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:10,858 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:10,860 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:59:16,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:17,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:18,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:19,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:20,864 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:21,865 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:22,865 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:23,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:24,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:25,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:25,873 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:59:31,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:32,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:33,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:34,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:35,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:36,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:37,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:38,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:39,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:40,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:40,887 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 08:59:46,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:47,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:48,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:49,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:50,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:51,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:52,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:53,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:54,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:55,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 08:59:55,897 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:00:01,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:02,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:03,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:04,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:05,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:06,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:07,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:08,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:09,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:10,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:10,913 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:00:16,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:17,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:18,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:19,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:20,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:21,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:22,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:23,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:24,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:25,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:25,923 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:00:31,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:32,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:33,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:34,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:35,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:36,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:37,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:38,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:39,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:40,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:40,936 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:00:46,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:47,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:48,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:49,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:50,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:51,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:52,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:53,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:54,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:55,943 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:00:55,945 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:01:01,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:02,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:03,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:04,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:05,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:06,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:07,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:08,952 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:09,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:10,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:10,956 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:01:16,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:17,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:18,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:19,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:20,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:21,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:22,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:23,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:24,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:25,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:25,964 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:01:31,965 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:32,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:33,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:34,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:35,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:36,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:37,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:38,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:39,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:40,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:40,977 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:01:46,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:47,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:48,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:49,980 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:50,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:51,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:52,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:53,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:54,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:55,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:01:55,985 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:02:01,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:02,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:03,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:04,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:05,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:06,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:07,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:08,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:09,992 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:10,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:10,996 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:02:16,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:17,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:18,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:19,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:20,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:22,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:23,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:24,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:25,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:26,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:26,005 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:02:32,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:33,007 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:34,007 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:35,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:36,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:37,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:38,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:39,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:40,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:41,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:41,016 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:02:47,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:48,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:49,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:50,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:51,019 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:52,019 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:53,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:54,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:55,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:56,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:02:56,028 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:03:02,029 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:03,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:04,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:05,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:06,032 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:07,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:08,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:09,034 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:10,034 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:11,035 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:11,037 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:03:17,038 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:18,038 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:19,039 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:20,039 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:21,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:22,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:23,041 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:24,041 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:25,042 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:26,043 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:26,047 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:03:32,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:33,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:34,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:35,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:36,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:37,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:38,055 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:39,055 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:40,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:41,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:41,060 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:03:47,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:48,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:49,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:50,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:51,063 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:52,063 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:53,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:54,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:55,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:56,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:03:56,070 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:04:02,071 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:03,071 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:04,072 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:05,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:06,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:07,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:08,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:09,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:10,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:11,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:11,080 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:04:17,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:18,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:19,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:20,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:21,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:22,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:23,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:24,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:25,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:26,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:26,089 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:04:32,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:33,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:34,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:35,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:36,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:37,094 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:38,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:39,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:40,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:41,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:41,099 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:04:47,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:48,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:49,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:50,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:51,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:52,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:53,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:54,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:55,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:56,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:04:56,106 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:05:02,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:03,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:04,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:05,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:06,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:07,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:08,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:09,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:10,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:11,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:11,117 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:05:17,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:18,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:19,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:20,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:21,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:22,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:23,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:24,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:25,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:26,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:26,124 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:05:32,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:33,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:34,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:35,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:36,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:37,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:38,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:39,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:40,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:41,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:41,134 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:05:47,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:48,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:49,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:50,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:51,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:52,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:53,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:54,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:55,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:56,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:05:56,144 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:06:02,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:03,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:04,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:05,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:06,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:07,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:08,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:09,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:10,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:11,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:11,152 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:06:17,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:18,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:19,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:20,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:21,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:22,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:23,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:24,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:25,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:26,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:26,162 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:06:32,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:33,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:34,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:35,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:36,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:37,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:38,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:39,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:40,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:41,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:41,170 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:06:47,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:48,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:49,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:50,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:51,174 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:52,174 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:53,175 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:54,175 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:55,176 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:56,176 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:06:56,179 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:07:02,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:03,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:04,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:05,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:06,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:07,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:08,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:09,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:10,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:11,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:11,189 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:07:17,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:18,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:19,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:20,192 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:21,192 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:22,193 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:23,193 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:24,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:25,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:26,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:26,198 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:07:32,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:33,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:34,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:35,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:36,201 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:37,201 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:38,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:39,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:40,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:41,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:41,206 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:07:47,207 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:48,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:49,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:50,209 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:51,209 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:52,210 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:53,211 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:54,211 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:55,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:56,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:07:56,215 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:08:02,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:03,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:04,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:05,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:06,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:07,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:08,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:09,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:10,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:11,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:11,223 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:08:17,224 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:18,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:19,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:20,226 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:21,226 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:22,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:23,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:24,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:25,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:26,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:26,232 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:08:32,233 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:33,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:34,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:35,235 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:36,235 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:37,236 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:38,236 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:39,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:40,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:41,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:41,241 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:08:47,242 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:48,242 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:49,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:50,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:51,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:52,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:53,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:54,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:55,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:56,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:08:56,248 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:09:02,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:03,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:04,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:05,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:06,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:07,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:08,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:09,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:10,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:11,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:11,256 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:09:17,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:18,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:19,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:20,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:21,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:22,260 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:23,260 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:24,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:25,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:26,262 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:26,265 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:09:32,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:33,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:34,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:35,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:36,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:37,269 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:38,269 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:39,270 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:40,270 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:41,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:41,272 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:09:47,273 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:48,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:49,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:50,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:51,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:52,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:53,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:54,277 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:55,277 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:56,278 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:09:56,281 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:10:02,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:03,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:04,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:05,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:06,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:07,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:08,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:09,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:10,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:11,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:11,288 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:10:17,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:18,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:19,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:20,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:21,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:22,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:23,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:24,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:25,293 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:26,293 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:26,299 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:10:32,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:33,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:34,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:35,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:36,302 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:37,302 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:38,303 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:39,303 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:40,304 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:41,304 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:41,307 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:10:47,308 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:48,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:49,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:50,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:51,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:52,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:53,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:54,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:55,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:56,313 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:10:56,315 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:11:02,316 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:03,316 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:04,317 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:05,317 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:06,318 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:07,318 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:08,319 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:09,319 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:10,320 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:11,320 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:11,324 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:11:17,325 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:18,325 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:19,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:20,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:21,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:22,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:23,328 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:24,328 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:25,329 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:26,330 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:26,334 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:11:32,335 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:33,335 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:34,336 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:35,336 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:36,336 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:37,337 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:38,337 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:39,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:40,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:41,339 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:41,342 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:11:47,343 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:48,343 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:49,344 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:50,344 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:51,345 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:52,345 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:53,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:54,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:55,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:56,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:11:56,349 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:12:02,350 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:03,350 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:04,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:05,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:06,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:07,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:08,353 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:09,353 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:10,353 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:11,354 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:11,357 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:12:17,358 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:18,358 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:19,359 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:20,359 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:21,360 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:22,360 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:23,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:24,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:25,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:26,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:26,365 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:12:32,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:33,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:34,367 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:35,367 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:36,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:37,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:38,369 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:39,369 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:40,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:41,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:41,373 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:12:47,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:48,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:49,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:50,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:51,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:52,377 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:53,377 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:54,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:55,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:56,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:12:56,380 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:13:02,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:03,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:04,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:05,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:06,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:07,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:08,384 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:09,384 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:10,385 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:11,385 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:11,387 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:13:17,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:18,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:19,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:20,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:21,390 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:22,390 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:23,391 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:24,391 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:25,391 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:26,392 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:26,396 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:13:32,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:33,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:34,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:35,399 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:36,399 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:37,400 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:38,400 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:39,401 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:40,401 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:41,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:41,403 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:13:47,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:48,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:49,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:50,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:51,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:52,407 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:53,407 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:54,408 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:55,408 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:56,409 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:13:56,410 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:14:02,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:03,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:04,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:05,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:06,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:07,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:08,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:09,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:10,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:11,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:11,418 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:14:17,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:18,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:19,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:20,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:21,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:22,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:23,422 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:24,422 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:25,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:26,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:26,426 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:14:32,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:33,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:34,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:35,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:36,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:37,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:38,430 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:39,430 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:40,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:41,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:41,434 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:14:47,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:48,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:49,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:50,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:51,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:52,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:53,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:54,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:55,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:56,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:14:56,441 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:15:02,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:03,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:04,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:05,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:06,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:07,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:08,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:09,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:10,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:11,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:11,449 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:15:17,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:18,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:19,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:20,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:21,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:22,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:23,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:24,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:25,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:26,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:26,456 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:15:32,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:33,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:34,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:35,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:36,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:37,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:38,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:39,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:40,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:41,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:41,465 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:15:47,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:48,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:49,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:50,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:51,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:52,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:53,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:54,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:55,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:56,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:15:56,472 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:16:02,473 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:03,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:04,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:05,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:06,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:07,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:08,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:09,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:10,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:11,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:11,480 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:16:17,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:18,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:19,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:20,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:21,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:22,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:23,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:24,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:25,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:26,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:26,489 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:16:32,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:33,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:34,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:35,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:36,492 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:37,492 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:38,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:39,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:40,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:41,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:41,496 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:16:47,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:48,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:49,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:50,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:51,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:52,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:53,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:54,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:55,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:56,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:16:56,504 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:17:02,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:03,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:04,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:05,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:06,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:07,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:08,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:09,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:10,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:11,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:11,511 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:17:17,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:18,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:19,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:20,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:21,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:22,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:23,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:24,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:25,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:26,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:26,519 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:17:32,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:33,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:34,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:35,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:36,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:37,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:38,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:39,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:40,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:41,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:41,527 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:17:47,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:48,530 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:49,530 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:50,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:51,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:52,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:53,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:54,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:55,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:56,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:17:56,536 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:18:02,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:03,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:04,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:05,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:06,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:07,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:08,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:09,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:10,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:11,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:11,544 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:18:17,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:18,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:19,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:20,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:21,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:22,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:23,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:24,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:25,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:26,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:26,552 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:18:32,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:33,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:34,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:35,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:36,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:37,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:38,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:39,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:40,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:41,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:41,559 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:18:47,560 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:48,560 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:49,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:50,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:51,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:52,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:53,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:54,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:55,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:56,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:18:56,566 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:19:02,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:19:03,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:19:04,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:19:05,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:19:06,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:19:07,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:19:08,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:19:09,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:19:10,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:19:11,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:19:11,574 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 09:19:17,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:19:18,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:19:19,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:19:20,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:19:21,579 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:19:21,750 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-09-23 09:19:21,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 09:20:42,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 09:20:42,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 09:20:43,267 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 09:20:43,392 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 09:20:43,393 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 09:20:43,398 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 09:20:43,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 09:20:43,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 09:20:43,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 09:20:43,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 09:20:43,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 09:20:43,597 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 09:20:43,610 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 09:20:43,625 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 09:20:43,641 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 09:20:43,643 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 09:20:43,643 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 09:20:43,643 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 09:20:43,664 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35673
2018-09-23 09:20:43,664 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 09:20:43,921 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35673
2018-09-23 09:20:44,064 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 09:20:44,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 09:20:44,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 09:20:44,564 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 09:20:44,596 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 09:20:44,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 09:20:44,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 09:20:44,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 09:20:44,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 09:20:44,757 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 09:20:44,758 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 09:20:45,233 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 09:20:45,241 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 6907@ip-172-31-87-173.ec2.internal
2018-09-23 09:20:45,242 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-ubuntu/dfs/data is not formatted for namespace 719224053. Formatting...
2018-09-23 09:20:45,243 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-23fee418-979e-4c1b-8141-e0458dcd4805 for directory /tmp/hadoop-ubuntu/dfs/data
2018-09-23 09:20:45,364 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1227375468-172.31.87.173-1537694423326
2018-09-23 09:20:45,364 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /tmp/hadoop-ubuntu/dfs/data/current/BP-1227375468-172.31.87.173-1537694423326
2018-09-23 09:20:45,368 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /tmp/hadoop-ubuntu/dfs/data/current/BP-1227375468-172.31.87.173-1537694423326 is not formatted for BP-1227375468-172.31.87.173-1537694423326. Formatting ...
2018-09-23 09:20:45,369 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1227375468-172.31.87.173-1537694423326 directory /tmp/hadoop-ubuntu/dfs/data/current/BP-1227375468-172.31.87.173-1537694423326/current
2018-09-23 09:20:45,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=719224053;bpid=BP-1227375468-172.31.87.173-1537694423326;lv=-56;nsInfo=lv=-63;cid=CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39;nsid=719224053;c=0;bpid=BP-1227375468-172.31.87.173-1537694423326;dnuuid=null
2018-09-23 09:20:45,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID f9c2d03b-4c32-4fc9-8790-e251107208b7
2018-09-23 09:20:45,448 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-23fee418-979e-4c1b-8141-e0458dcd4805
2018-09-23 09:20:45,448 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-ubuntu/dfs/data/current, StorageType: DISK
2018-09-23 09:20:45,455 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-09-23 09:20:45,455 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1227375468-172.31.87.173-1537694423326
2018-09-23 09:20:45,457 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1227375468-172.31.87.173-1537694423326 on volume /tmp/hadoop-ubuntu/dfs/data/current...
2018-09-23 09:20:45,471 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1227375468-172.31.87.173-1537694423326 on /tmp/hadoop-ubuntu/dfs/data/current: 14ms
2018-09-23 09:20:45,471 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1227375468-172.31.87.173-1537694423326: 15ms
2018-09-23 09:20:45,472 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1227375468-172.31.87.173-1537694423326 on volume /tmp/hadoop-ubuntu/dfs/data/current...
2018-09-23 09:20:45,473 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1227375468-172.31.87.173-1537694423326 on volume /tmp/hadoop-ubuntu/dfs/data/current: 0ms
2018-09-23 09:20:45,473 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2018-09-23 09:20:45,474 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1227375468-172.31.87.173-1537694423326 on volume /tmp/hadoop-ubuntu/dfs/data
2018-09-23 09:20:45,475 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-ubuntu/dfs/data, DS-23fee418-979e-4c1b-8141-e0458dcd4805): finished scanning block pool BP-1227375468-172.31.87.173-1537694423326
2018-09-23 09:20:45,481 ERROR org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: dfs.datanode.directoryscan.throttle.limit.ms.per.sec set to value below 1 ms/sec. Assuming default value of 1000
2018-09-23 09:20:45,481 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1537700558481ms with interval of 21600000ms
2018-09-23 09:20:45,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1227375468-172.31.87.173-1537694423326 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2018-09-23 09:20:45,586 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-ubuntu/dfs/data, DS-23fee418-979e-4c1b-8141-e0458dcd4805): no suitable block pools found to scan.  Waiting 1814399888 ms.
2018-09-23 09:20:45,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1227375468-172.31.87.173-1537694423326 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2018-09-23 09:20:45,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-09-23 09:20:45,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1227375468-172.31.87.173-1537694423326 (Datanode Uuid f9c2d03b-4c32-4fc9-8790-e251107208b7) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2018-09-23 09:20:45,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1227375468-172.31.87.173-1537694423326 (Datanode Uuid f9c2d03b-4c32-4fc9-8790-e251107208b7) service to localhost/127.0.0.1:9000
2018-09-23 09:20:45,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x23670c68a80,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 2 msec to generate and 143 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-23 09:20:45,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1227375468-172.31.87.173-1537694423326
2018-09-23 09:24:10,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1227375468-172.31.87.173-1537694423326:blk_1073741825_1001 src: /127.0.0.1:40068 dest: /127.0.0.1:50010
2018-09-23 09:24:10,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:40068, dest: /127.0.0.1:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1988462562_1, offset: 0, srvID: f9c2d03b-4c32-4fc9-8790-e251107208b7, blockid: BP-1227375468-172.31.87.173-1537694423326:blk_1073741825_1001, duration: 121113904
2018-09-23 09:24:10,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1227375468-172.31.87.173-1537694423326:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 09:24:12,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1227375468-172.31.87.173-1537694423326:blk_1073741826_1002 src: /127.0.0.1:40072 dest: /127.0.0.1:50010
2018-09-23 09:24:12,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:40072, dest: /127.0.0.1:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1988462562_1, offset: 0, srvID: f9c2d03b-4c32-4fc9-8790-e251107208b7, blockid: BP-1227375468-172.31.87.173-1537694423326:blk_1073741826_1002, duration: 1138023
2018-09-23 09:24:12,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1227375468-172.31.87.173-1537694423326:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 09:24:12,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1227375468-172.31.87.173-1537694423326:blk_1073741827_1003 src: /127.0.0.1:40074 dest: /127.0.0.1:50010
2018-09-23 09:24:12,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:40074, dest: /127.0.0.1:50010, bytes: 97, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1988462562_1, offset: 0, srvID: f9c2d03b-4c32-4fc9-8790-e251107208b7, blockid: BP-1227375468-172.31.87.173-1537694423326:blk_1073741827_1003, duration: 1345364
2018-09-23 09:24:12,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1227375468-172.31.87.173-1537694423326:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 09:24:15,632 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /tmp/hadoop-ubuntu/dfs/data/current/BP-1227375468-172.31.87.173-1537694423326/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2018-09-23 09:24:15,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /tmp/hadoop-ubuntu/dfs/data/current/BP-1227375468-172.31.87.173-1537694423326/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2018-09-23 09:24:15,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /tmp/hadoop-ubuntu/dfs/data/current/BP-1227375468-172.31.87.173-1537694423326/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2018-09-23 09:24:15,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1227375468-172.31.87.173-1537694423326 blk_1073741825_1001 file /tmp/hadoop-ubuntu/dfs/data/current/BP-1227375468-172.31.87.173-1537694423326/current/finalized/subdir0/subdir0/blk_1073741825
2018-09-23 09:24:15,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1227375468-172.31.87.173-1537694423326 blk_1073741826_1002 file /tmp/hadoop-ubuntu/dfs/data/current/BP-1227375468-172.31.87.173-1537694423326/current/finalized/subdir0/subdir0/blk_1073741826
2018-09-23 09:24:15,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1227375468-172.31.87.173-1537694423326 blk_1073741827_1003 file /tmp/hadoop-ubuntu/dfs/data/current/BP-1227375468-172.31.87.173-1537694423326/current/finalized/subdir0/subdir0/blk_1073741827
2018-09-23 09:36:27,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1227375468-172.31.87.173-1537694423326:blk_1073741828_1004 src: /127.0.0.1:40124 dest: /127.0.0.1:50010
2018-09-23 09:36:27,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:40124, dest: /127.0.0.1:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_569477869_1, offset: 0, srvID: f9c2d03b-4c32-4fc9-8790-e251107208b7, blockid: BP-1227375468-172.31.87.173-1537694423326:blk_1073741828_1004, duration: 61104476
2018-09-23 09:36:27,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1227375468-172.31.87.173-1537694423326:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 09:36:29,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1227375468-172.31.87.173-1537694423326:blk_1073741829_1005 src: /127.0.0.1:40128 dest: /127.0.0.1:50010
2018-09-23 09:36:29,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:40128, dest: /127.0.0.1:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_569477869_1, offset: 0, srvID: f9c2d03b-4c32-4fc9-8790-e251107208b7, blockid: BP-1227375468-172.31.87.173-1537694423326:blk_1073741829_1005, duration: 1698415
2018-09-23 09:36:29,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1227375468-172.31.87.173-1537694423326:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 09:36:29,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1227375468-172.31.87.173-1537694423326:blk_1073741830_1006 src: /127.0.0.1:40130 dest: /127.0.0.1:50010
2018-09-23 09:36:29,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:40130, dest: /127.0.0.1:50010, bytes: 97, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_569477869_1, offset: 0, srvID: f9c2d03b-4c32-4fc9-8790-e251107208b7, blockid: BP-1227375468-172.31.87.173-1537694423326:blk_1073741830_1006, duration: 1361359
2018-09-23 09:36:29,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1227375468-172.31.87.173-1537694423326:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 09:36:33,656 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /tmp/hadoop-ubuntu/dfs/data/current/BP-1227375468-172.31.87.173-1537694423326/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2018-09-23 09:36:33,660 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /tmp/hadoop-ubuntu/dfs/data/current/BP-1227375468-172.31.87.173-1537694423326/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2018-09-23 09:36:33,660 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /tmp/hadoop-ubuntu/dfs/data/current/BP-1227375468-172.31.87.173-1537694423326/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2018-09-23 09:36:33,660 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1227375468-172.31.87.173-1537694423326 blk_1073741828_1004 file /tmp/hadoop-ubuntu/dfs/data/current/BP-1227375468-172.31.87.173-1537694423326/current/finalized/subdir0/subdir0/blk_1073741828
2018-09-23 09:36:33,660 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1227375468-172.31.87.173-1537694423326 blk_1073741829_1005 file /tmp/hadoop-ubuntu/dfs/data/current/BP-1227375468-172.31.87.173-1537694423326/current/finalized/subdir0/subdir0/blk_1073741829
2018-09-23 09:36:33,660 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1227375468-172.31.87.173-1537694423326 blk_1073741830_1006 file /tmp/hadoop-ubuntu/dfs/data/current/BP-1227375468-172.31.87.173-1537694423326/current/finalized/subdir0/subdir0/blk_1073741830
2018-09-23 09:58:15,751 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ip-172-31-87-173.ec2.internal/172.31.87.173"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1413)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:152)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:402)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:659)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1085)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:980)
2018-09-23 09:58:19,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:20,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:21,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:22,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:23,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:24,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:25,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:26,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:27,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:28,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:28,757 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ip-172-31-87-173.ec2.internal/172.31.87.173 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1413)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:152)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:402)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:659)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:615)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:376)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1452)
	... 8 more
2018-09-23 09:58:29,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:30,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:31,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:32,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:33,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:34,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:35,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:36,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:37,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:38,765 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:38,767 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ip-172-31-87-173.ec2.internal/172.31.87.173 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1413)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:152)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:402)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:659)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:615)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:376)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1452)
	... 8 more
2018-09-23 09:58:39,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:40,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:41,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:42,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:43,774 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:44,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:45,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:46,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:47,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:48,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:48,784 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ip-172-31-87-173.ec2.internal/172.31.87.173 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1413)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:152)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:402)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:659)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:615)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:376)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1452)
	... 8 more
2018-09-23 09:58:49,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:50,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:51,790 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:52,790 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:53,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:54,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:55,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:56,793 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:57,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:58,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 09:58:58,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ip-172-31-87-173.ec2.internal/172.31.87.173 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1413)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:152)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:402)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:659)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:615)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:376)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1452)
	... 8 more
2018-09-23 09:58:59,376 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-09-23 09:58:59,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 10:00:21,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 10:00:21,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 10:00:22,405 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 10:00:22,521 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 10:00:22,522 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 10:00:22,528 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 10:00:22,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 10:00:22,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 10:00:22,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 10:00:22,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 10:00:22,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 10:00:22,725 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 10:00:22,738 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 10:00:22,769 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 10:00:22,781 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 10:00:22,783 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 10:00:22,783 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 10:00:22,783 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 10:00:22,802 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45385
2018-09-23 10:00:22,802 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 10:00:23,050 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45385
2018-09-23 10:00:23,220 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 10:00:23,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 10:00:23,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 10:00:23,728 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 10:00:23,765 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 10:00:23,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 10:00:23,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 10:00:23,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 10:00:23,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 10:00:23,972 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 10:00:23,977 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 10:00:25,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:26,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:27,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:28,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:29,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:30,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:31,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:32,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:33,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:34,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:34,126 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:00:40,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:41,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:42,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:43,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:44,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:45,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:46,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:47,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:48,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:49,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:49,135 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:00:55,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:56,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:57,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:58,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:00:59,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:00,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:01,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:02,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:03,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:04,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:04,146 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:01:10,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:11,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:12,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:13,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:14,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:15,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:16,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:17,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:18,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:19,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:19,155 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:01:25,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:26,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:27,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:28,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:29,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:30,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:31,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:32,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:33,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:34,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:34,166 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:01:40,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:41,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:42,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:43,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:44,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:45,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:46,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:47,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:48,174 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:49,175 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:49,177 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:01:55,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:56,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:57,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:58,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:01:59,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:00,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:01,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:02,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:03,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:04,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:04,188 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:02:10,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:11,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:12,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:13,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:14,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:15,192 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:16,193 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:17,193 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:18,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:19,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:19,196 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:02:25,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:26,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:27,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:28,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:29,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:30,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:31,201 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:32,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:33,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:34,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:34,205 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:02:40,206 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:41,206 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:42,207 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:43,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:44,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:45,209 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:46,209 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:47,210 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:48,211 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:49,211 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:49,213 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:02:55,215 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:56,215 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:57,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:58,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:02:59,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:00,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:01,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:02,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:03,221 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:04,222 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:04,227 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:03:10,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:11,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:12,230 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:13,230 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:14,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:15,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:16,232 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:17,233 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:18,233 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:19,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:19,236 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:03:25,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:26,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:27,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:28,239 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:29,240 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:30,240 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:31,241 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:32,242 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:33,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:34,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:34,245 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:03:40,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:41,247 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:42,247 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:43,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:44,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:45,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:46,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:47,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:48,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:49,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:49,254 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:03:55,255 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:56,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:57,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:58,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:03:59,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:04:00,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:04:01,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:04:02,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:04:03,260 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:04:04,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:04:04,264 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:04:10,265 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:04:11,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:04:11,342 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-09-23 10:04:11,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 10:05:44,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 10:05:44,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 10:05:45,256 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 10:05:45,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 10:05:45,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 10:05:45,386 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 10:05:45,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 10:05:45,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 10:05:45,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 10:05:45,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 10:05:45,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 10:05:45,574 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 10:05:45,586 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 10:05:45,616 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 10:05:45,626 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 10:05:45,628 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 10:05:45,628 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 10:05:45,628 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 10:05:45,649 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35541
2018-09-23 10:05:45,649 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 10:05:45,897 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35541
2018-09-23 10:05:46,048 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 10:05:46,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 10:05:46,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 10:05:46,516 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 10:05:46,547 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 10:05:46,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 10:05:46,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 10:05:46,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 10:05:46,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 10:05:46,702 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 10:05:46,705 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 10:05:47,429 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 10:05:47,450 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 11571@ip-172-31-87-173.ec2.internal
2018-09-23 10:05:47,453 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data/
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-b6fadfbb-7deb-4c28-bbc6-b0393f55bbfb; datanode clusterID = CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:777)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:300)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:416)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:395)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:573)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:05:47,469 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:05:47,473 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2018-09-23 10:05:47,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2018-09-23 10:05:49,480 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2018-09-23 10:05:49,481 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2018-09-23 10:05:49,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 10:07:15,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 10:07:15,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 10:07:16,089 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 10:07:16,216 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 10:07:16,217 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 10:07:16,226 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 10:07:16,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 10:07:16,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 10:07:16,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 10:07:16,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 10:07:16,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 10:07:16,400 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 10:07:16,417 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 10:07:16,442 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 10:07:16,453 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 10:07:16,455 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 10:07:16,455 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 10:07:16,457 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 10:07:16,477 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 32867
2018-09-23 10:07:16,477 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 10:07:16,702 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:32867
2018-09-23 10:07:16,836 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 10:07:17,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 10:07:17,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 10:07:17,353 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 10:07:17,381 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 10:07:17,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 10:07:17,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 10:07:17,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 10:07:17,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 10:07:17,570 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 10:07:17,574 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 10:07:17,872 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 10:07:17,884 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 12709@ip-172-31-87-173.ec2.internal
2018-09-23 10:07:17,886 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data/
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-b6fadfbb-7deb-4c28-bbc6-b0393f55bbfb; datanode clusterID = CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:777)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:300)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:416)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:395)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:573)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:07:17,893 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:07:17,893 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2018-09-23 10:07:17,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2018-09-23 10:07:19,895 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2018-09-23 10:07:19,896 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2018-09-23 10:07:19,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 10:07:45,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 10:07:45,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 10:07:46,517 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 10:07:46,638 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 10:07:46,638 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 10:07:46,645 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 10:07:46,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 10:07:46,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 10:07:46,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 10:07:46,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 10:07:46,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 10:07:46,840 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 10:07:46,854 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 10:07:46,880 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 10:07:46,893 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 10:07:46,895 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 10:07:46,895 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 10:07:46,895 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 10:07:46,918 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45012
2018-09-23 10:07:46,918 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 10:07:47,161 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45012
2018-09-23 10:07:47,299 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 10:07:47,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 10:07:47,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 10:07:47,807 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 10:07:47,842 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 10:07:47,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 10:07:47,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 10:07:47,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 10:07:48,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 10:07:48,021 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 10:07:48,025 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 10:07:48,337 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 10:07:48,343 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 13317@ip-172-31-87-173.ec2.internal
2018-09-23 10:07:48,344 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data/
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-b6fadfbb-7deb-4c28-bbc6-b0393f55bbfb; datanode clusterID = CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:777)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:300)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:416)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:395)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:573)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:07:48,349 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:07:48,349 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2018-09-23 10:07:48,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2018-09-23 10:07:50,353 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2018-09-23 10:07:50,355 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2018-09-23 10:07:50,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 10:09:00,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 10:09:00,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 10:09:01,760 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 10:09:01,888 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 10:09:01,889 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 10:09:01,894 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 10:09:01,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 10:09:01,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 10:09:01,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 10:09:01,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 10:09:01,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 10:09:02,088 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 10:09:02,101 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 10:09:02,128 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 10:09:02,140 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 10:09:02,142 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 10:09:02,142 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 10:09:02,145 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 10:09:02,163 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42414
2018-09-23 10:09:02,163 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 10:09:02,373 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42414
2018-09-23 10:09:02,512 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 10:09:02,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 10:09:02,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 10:09:03,016 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 10:09:03,046 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 10:09:03,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 10:09:03,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 10:09:03,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 10:09:03,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 10:09:03,237 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 10:09:03,241 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 10:09:03,752 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 10:09:03,766 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 14495@ip-172-31-87-173.ec2.internal
2018-09-23 10:09:03,768 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data/
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-b6fadfbb-7deb-4c28-bbc6-b0393f55bbfb; datanode clusterID = CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:777)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:300)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:416)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:395)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:573)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:09:03,777 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:09:03,777 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2018-09-23 10:09:03,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2018-09-23 10:09:05,782 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2018-09-23 10:09:05,783 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2018-09-23 10:09:05,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 10:10:56,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 10:10:56,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 10:10:57,344 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 10:10:57,472 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 10:10:57,473 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 10:10:57,478 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 10:10:57,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 10:10:57,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 10:10:57,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 10:10:57,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 10:10:57,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 10:10:57,668 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 10:10:57,680 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 10:10:57,708 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 10:10:57,720 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 10:10:57,722 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 10:10:57,722 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 10:10:57,722 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 10:10:57,745 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37320
2018-09-23 10:10:57,746 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 10:10:57,965 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37320
2018-09-23 10:10:58,108 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 10:10:58,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 10:10:58,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 10:10:58,617 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 10:10:58,650 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 10:10:58,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 10:10:58,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 10:10:58,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 10:10:58,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 10:10:58,836 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 10:10:58,837 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 10:10:59,353 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 10:10:59,366 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 16169@ip-172-31-87-173.ec2.internal
2018-09-23 10:10:59,368 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data/
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-b6fadfbb-7deb-4c28-bbc6-b0393f55bbfb; datanode clusterID = CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:777)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:300)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:416)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:395)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:573)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:10:59,375 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:10:59,375 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2018-09-23 10:10:59,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2018-09-23 10:11:01,377 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2018-09-23 10:11:01,378 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2018-09-23 10:11:01,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 10:14:15,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 10:14:15,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 10:14:16,581 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 10:14:16,709 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 10:14:16,709 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 10:14:16,717 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 10:14:16,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 10:14:16,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 10:14:16,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 10:14:16,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 10:14:16,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 10:14:16,913 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 10:14:16,929 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 10:14:16,957 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 10:14:16,962 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 10:14:16,966 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 10:14:16,966 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 10:14:16,966 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 10:14:16,976 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 40630
2018-09-23 10:14:16,976 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 10:14:17,189 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:40630
2018-09-23 10:14:17,335 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 10:14:17,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 10:14:17,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 10:14:17,853 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 10:14:17,883 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 10:14:17,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 10:14:17,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 10:14:18,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 10:14:18,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 10:14:18,065 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 10:14:18,069 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 10:14:18,503 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 10:14:18,510 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 18107@ip-172-31-87-173.ec2.internal
2018-09-23 10:14:18,512 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data/
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-2b6b47e1-1493-4e48-8fd6-c6d6f5012d9c; datanode clusterID = CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:777)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:300)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:416)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:395)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:573)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:14:18,518 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:14:18,518 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2018-09-23 10:14:18,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2018-09-23 10:14:20,520 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2018-09-23 10:14:20,525 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2018-09-23 10:14:20,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 10:17:17,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 10:17:17,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 10:17:18,642 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 10:17:18,795 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 10:17:18,795 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 10:17:18,805 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 10:17:18,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 10:17:18,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 10:17:18,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 10:17:18,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 10:17:18,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 10:17:18,997 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 10:17:19,010 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 10:17:19,039 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 10:17:19,049 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 10:17:19,055 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 10:17:19,055 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 10:17:19,055 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 10:17:19,075 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41318
2018-09-23 10:17:19,075 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 10:17:19,293 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41318
2018-09-23 10:17:19,432 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 10:17:19,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 10:17:19,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 10:17:19,948 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 10:17:19,981 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 10:17:20,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 10:17:20,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 10:17:20,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 10:17:20,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 10:17:20,150 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 10:17:20,156 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 10:17:20,615 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 10:17:20,625 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 20398@ip-172-31-87-173.ec2.internal
2018-09-23 10:17:20,627 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data/
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-2b6b47e1-1493-4e48-8fd6-c6d6f5012d9c; datanode clusterID = CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:777)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:300)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:416)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:395)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:573)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:17:20,634 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:17:20,634 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2018-09-23 10:17:20,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2018-09-23 10:17:22,636 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2018-09-23 10:17:22,637 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2018-09-23 10:17:22,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 10:26:07,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 10:26:07,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 10:26:08,991 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 10:26:09,116 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 10:26:09,117 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 10:26:09,122 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 10:26:09,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 10:26:09,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 10:26:09,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 10:26:09,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 10:26:09,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 10:26:09,320 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 10:26:09,334 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 10:26:09,363 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 10:26:09,372 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 10:26:09,378 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 10:26:09,378 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 10:26:09,378 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 10:26:09,398 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34873
2018-09-23 10:26:09,398 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 10:26:09,621 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34873
2018-09-23 10:26:09,766 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 10:26:10,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 10:26:10,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 10:26:10,300 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 10:26:10,329 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 10:26:10,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 10:26:10,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 10:26:10,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 10:26:10,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 10:26:10,497 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 10:26:10,501 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 10:26:10,769 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 10:26:10,779 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 21301@ip-172-31-87-173.ec2.internal
2018-09-23 10:26:10,781 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data/
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-2b6b47e1-1493-4e48-8fd6-c6d6f5012d9c; datanode clusterID = CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:777)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:300)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:416)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:395)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:573)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:26:10,789 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 10:26:10,789 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2018-09-23 10:26:10,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2018-09-23 10:26:12,791 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2018-09-23 10:26:12,792 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2018-09-23 10:26:12,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 10:38:13,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 10:38:13,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 10:38:14,104 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 10:38:14,232 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 10:38:14,233 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 10:38:14,238 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 10:38:14,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 10:38:14,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 10:38:14,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 10:38:14,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 10:38:14,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 10:38:14,443 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 10:38:14,458 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 10:38:14,484 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 10:38:14,495 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 10:38:14,497 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 10:38:14,497 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 10:38:14,501 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 10:38:14,522 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42388
2018-09-23 10:38:14,522 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 10:38:14,769 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42388
2018-09-23 10:38:14,900 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 10:38:15,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 10:38:15,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 10:38:15,368 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 10:38:15,398 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 10:38:15,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 10:38:15,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 10:38:15,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 10:38:15,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 10:38:15,584 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 10:38:15,589 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 10:38:16,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:17,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:18,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:19,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:20,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:21,721 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:22,722 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:23,722 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:24,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:25,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:25,727 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:38:31,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:32,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:33,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:34,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:35,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:36,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:37,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:38,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:39,735 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:40,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:40,738 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:38:46,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:47,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:48,740 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:49,741 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:50,741 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:51,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:52,745 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:53,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:54,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:55,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:38:55,750 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:39:01,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:02,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:03,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:04,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:05,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:06,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:07,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:08,756 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:09,757 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:10,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:10,761 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:39:16,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:17,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:18,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:19,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:20,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:21,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:22,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:23,768 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:24,769 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:25,769 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:25,774 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:39:31,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:32,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:33,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:34,777 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:35,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:36,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:37,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:38,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:39,782 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:40,783 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:40,784 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:39:46,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:47,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:48,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:49,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:50,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:51,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:52,790 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:53,793 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:54,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:55,795 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:39:55,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:40:01,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:02,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:03,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:04,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:05,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:06,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:07,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:08,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:09,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:10,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:10,807 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:40:16,808 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:17,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:18,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:19,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:20,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:21,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:22,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:23,814 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:24,814 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:25,815 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:25,819 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:40:31,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:32,821 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:33,821 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:34,822 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:35,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:36,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:37,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:38,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:39,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:40,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:40,829 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:40:46,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:47,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:48,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:49,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:50,833 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:51,837 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:52,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:53,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:54,839 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:55,840 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:40:55,844 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:41:01,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:02,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:03,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:04,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:05,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:06,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:07,854 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:08,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:09,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:10,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:10,858 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:41:16,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:17,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:18,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:19,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:20,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:21,864 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:22,865 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:23,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:24,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:25,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:25,871 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:41:31,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:32,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:33,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:34,874 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:35,874 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:36,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:37,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:38,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:39,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:40,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:40,879 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:41:46,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:47,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:48,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:49,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:50,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:51,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:52,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:53,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:54,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:55,887 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:41:55,889 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:42:01,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:42:02,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:42:03,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:42:04,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:42:05,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:42:06,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:42:07,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:42:08,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:42:09,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:42:10,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:42:10,900 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 10:42:16,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:42:17,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 10:42:18,234 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-09-23 10:42:18,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 11:36:32,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 11:36:32,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 11:36:33,986 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 11:36:34,104 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 11:36:34,105 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 11:36:34,110 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 11:36:34,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 11:36:34,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 11:36:34,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 11:36:34,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 11:36:34,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 11:36:34,308 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 11:36:34,320 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 11:36:34,349 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 11:36:34,357 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 11:36:34,359 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 11:36:34,359 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 11:36:34,359 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 11:36:34,382 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 43677
2018-09-23 11:36:34,382 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 11:36:34,617 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43677
2018-09-23 11:36:34,756 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 11:36:35,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 11:36:35,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 11:36:35,272 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 11:36:35,303 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 11:36:35,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 11:36:35,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 11:36:35,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 11:36:35,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 11:36:35,497 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 11:36:35,501 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 11:36:36,645 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:37,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:38,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:39,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:40,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:41,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:42,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:43,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:44,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:45,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:45,654 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 11:36:51,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:52,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:53,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:54,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:55,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:56,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:57,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:58,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:36:59,660 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:00,660 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:00,663 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 11:37:06,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:07,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:08,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:09,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:10,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:11,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:12,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:13,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:14,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:15,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:15,676 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 11:37:21,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:22,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:23,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:24,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:25,681 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:26,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:27,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:28,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:29,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:30,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:30,688 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 11:37:36,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:37,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:38,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:39,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:40,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:41,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:42,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:43,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:44,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:45,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:45,698 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 11:37:51,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:52,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:53,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:54,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:55,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:56,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:57,703 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:58,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:37:59,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:00,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:00,709 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 11:38:06,710 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:07,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:08,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:09,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:10,713 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:11,713 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:12,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:13,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:14,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:15,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:15,718 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 11:38:21,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:22,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:23,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:24,721 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:25,721 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:26,722 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:27,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:28,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:29,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:30,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:30,730 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-09-23 11:38:36,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-09-23 11:38:37,098 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 11:38:37,109 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 24501@ip-172-31-87-173.ec2.internal
2018-09-23 11:38:37,110 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data/
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-d001c3f0-c5b4-4d84-9719-ac689d4c33e7; datanode clusterID = CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:777)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:300)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:416)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:395)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:573)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 11:38:37,115 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 11:38:37,116 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2018-09-23 11:38:37,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2018-09-23 11:38:39,121 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2018-09-23 11:38:39,122 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2018-09-23 11:38:39,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 11:40:00,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 11:40:00,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 11:40:01,918 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 11:40:02,048 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 11:40:02,049 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 11:40:02,054 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 11:40:02,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 11:40:02,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 11:40:02,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 11:40:02,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 11:40:02,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 11:40:02,244 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 11:40:02,260 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 11:40:02,285 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 11:40:02,291 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 11:40:02,293 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 11:40:02,293 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 11:40:02,293 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 11:40:02,314 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37829
2018-09-23 11:40:02,314 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 11:40:02,580 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37829
2018-09-23 11:40:02,739 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 11:40:03,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 11:40:03,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 11:40:03,229 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 11:40:03,257 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 11:40:03,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 11:40:03,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 11:40:03,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 11:40:03,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 11:40:03,423 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 11:40:03,424 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 11:40:03,746 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 11:40:03,756 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 25826@ip-172-31-87-173.ec2.internal
2018-09-23 11:40:03,758 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data/
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-d001c3f0-c5b4-4d84-9719-ac689d4c33e7; datanode clusterID = CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:777)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:300)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:416)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:395)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:573)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 11:40:03,765 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 11:40:03,765 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2018-09-23 11:40:03,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2018-09-23 11:40:05,767 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2018-09-23 11:40:05,768 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2018-09-23 11:40:05,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 11:42:08,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 11:42:08,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 11:42:09,638 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 11:42:09,768 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 11:42:09,769 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 11:42:09,774 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 11:42:09,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 11:42:09,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 11:42:09,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 11:42:09,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 11:42:09,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 11:42:09,966 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 11:42:09,976 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 11:42:10,008 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 11:42:10,014 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 11:42:10,016 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 11:42:10,016 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 11:42:10,016 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 11:42:10,038 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35391
2018-09-23 11:42:10,038 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 11:42:10,294 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35391
2018-09-23 11:42:10,454 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 11:42:10,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 11:42:10,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 11:42:10,933 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 11:42:10,962 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 11:42:11,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 11:42:11,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 11:42:11,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 11:42:11,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 11:42:11,146 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 11:42:11,150 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 11:42:11,660 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 11:42:11,669 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 27266@ip-172-31-87-173.ec2.internal
2018-09-23 11:42:11,670 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data/
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-6b461683-e3b3-460a-a17e-39c8af09209b; datanode clusterID = CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:777)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:300)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:416)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:395)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:573)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 11:42:11,678 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 11:42:11,678 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2018-09-23 11:42:11,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2018-09-23 11:42:13,683 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2018-09-23 11:42:13,685 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2018-09-23 11:42:13,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 11:44:34,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 11:44:34,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 11:44:35,681 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 11:44:35,804 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 11:44:35,805 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 11:44:35,810 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 11:44:35,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 11:44:35,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 11:44:35,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 11:44:35,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 11:44:35,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 11:44:35,993 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 11:44:36,006 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 11:44:36,037 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 11:44:36,045 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 11:44:36,047 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 11:44:36,047 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 11:44:36,047 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 11:44:36,070 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37862
2018-09-23 11:44:36,070 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 11:44:36,317 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37862
2018-09-23 11:44:36,460 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 11:44:36,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 11:44:36,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 11:44:36,934 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 11:44:36,964 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 11:44:37,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 11:44:37,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 11:44:37,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 11:44:37,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 11:44:37,127 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 11:44:37,132 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 11:44:37,583 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 11:44:37,592 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 28970@ip-172-31-87-173.ec2.internal
2018-09-23 11:44:37,594 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data/
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-fa0c6fdd-97fa-47be-9906-9db49287bbb9; datanode clusterID = CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:777)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:300)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:416)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:395)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:573)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 11:44:37,601 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 11:44:37,602 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2018-09-23 11:44:37,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2018-09-23 11:44:39,604 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2018-09-23 11:44:39,606 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2018-09-23 11:44:39,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 11:50:09,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 11:50:09,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 11:50:10,780 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 11:50:10,896 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 11:50:10,896 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 11:50:10,902 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 11:50:10,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 11:50:10,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 11:50:10,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 11:50:10,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 11:50:10,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 11:50:11,085 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 11:50:11,098 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 11:50:11,128 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 11:50:11,139 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 11:50:11,141 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 11:50:11,141 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 11:50:11,141 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 11:50:11,166 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 43331
2018-09-23 11:50:11,166 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 11:50:11,412 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43331
2018-09-23 11:50:11,560 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 11:50:11,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 11:50:11,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 11:50:12,074 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 11:50:12,106 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 11:50:12,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 11:50:12,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 11:50:12,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 11:50:12,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 11:50:12,264 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 11:50:12,271 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 11:50:12,733 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 11:50:12,740 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 30671@ip-172-31-87-173.ec2.internal
2018-09-23 11:50:12,741 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data/
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-03577d93-0fc9-4795-9d29-a568f6f806e4; datanode clusterID = CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:777)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:300)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:416)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:395)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:573)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 11:50:12,749 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 11:50:12,749 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2018-09-23 11:50:12,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2018-09-23 11:50:14,751 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2018-09-23 11:50:14,752 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2018-09-23 11:50:14,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 11:56:27,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 11:56:27,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 11:56:28,131 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 11:56:28,250 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 11:56:28,250 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 11:56:28,258 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 11:56:28,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 11:56:28,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 11:56:28,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 11:56:28,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 11:56:28,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 11:56:28,464 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 11:56:28,472 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 11:56:28,497 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 11:56:28,503 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 11:56:28,505 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 11:56:28,505 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 11:56:28,505 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 11:56:28,516 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35497
2018-09-23 11:56:28,516 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 11:56:28,733 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35497
2018-09-23 11:56:28,880 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 11:56:29,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 11:56:29,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 11:56:29,430 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 11:56:29,458 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 11:56:29,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 11:56:29,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 11:56:29,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 11:56:29,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 11:56:29,624 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 11:56:29,625 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 11:56:29,921 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 11:56:29,928 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 31780@ip-172-31-87-173.ec2.internal
2018-09-23 11:56:29,929 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/tmp/hadoop-ubuntu/dfs/data/
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-ubuntu/dfs/data: namenode clusterID = CID-03577d93-0fc9-4795-9d29-a568f6f806e4; datanode clusterID = CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:777)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:300)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:416)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:395)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:573)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 11:56:29,936 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1393)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1358)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:313)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:216)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:637)
	at java.lang.Thread.run(Thread.java:748)
2018-09-23 11:56:29,936 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2018-09-23 11:56:29,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2018-09-23 11:56:31,938 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2018-09-23 11:56:31,939 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2018-09-23 11:56:31,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-09-23 12:21:54,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-09-23 12:21:54,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-09-23 12:21:55,546 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-09-23 12:21:55,691 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-09-23 12:21:55,691 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-09-23 12:21:55,696 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-09-23 12:21:55,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-09-23 12:21:55,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-09-23 12:21:55,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-09-23 12:21:55,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-09-23 12:21:55,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-09-23 12:21:55,888 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-09-23 12:21:55,896 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-09-23 12:21:55,925 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-09-23 12:21:55,935 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-09-23 12:21:55,937 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-09-23 12:21:55,937 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-09-23 12:21:55,939 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-09-23 12:21:55,958 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37780
2018-09-23 12:21:55,958 INFO org.mortbay.log: jetty-6.1.26
2018-09-23 12:21:56,209 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37780
2018-09-23 12:21:56,356 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-09-23 12:21:56,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-09-23 12:21:56,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-09-23 12:21:56,817 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-09-23 12:21:56,848 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-09-23 12:21:56,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-09-23 12:21:56,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-09-23 12:21:56,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-09-23 12:21:56,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-09-23 12:21:56,991 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-09-23 12:21:56,992 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-09-23 12:21:57,443 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-09-23 12:21:57,450 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ubuntu/dfs/data/in_use.lock acquired by nodename 750@ip-172-31-87-173.ec2.internal
2018-09-23 12:21:57,581 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-880181939-172.31.87.173-1537705260947
2018-09-23 12:21:57,581 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /tmp/hadoop-ubuntu/dfs/data/current/BP-880181939-172.31.87.173-1537705260947
2018-09-23 12:21:57,581 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /tmp/hadoop-ubuntu/dfs/data/current/BP-880181939-172.31.87.173-1537705260947 is not formatted for BP-880181939-172.31.87.173-1537705260947. Formatting ...
2018-09-23 12:21:57,581 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-880181939-172.31.87.173-1537705260947 directory /tmp/hadoop-ubuntu/dfs/data/current/BP-880181939-172.31.87.173-1537705260947/current
2018-09-23 12:21:57,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=2036785150;bpid=BP-880181939-172.31.87.173-1537705260947;lv=-56;nsInfo=lv=-63;cid=CID-39247b98-9b55-4e5d-b5e4-9fd8d02b5a39;nsid=2036785150;c=0;bpid=BP-880181939-172.31.87.173-1537705260947;dnuuid=f9c2d03b-4c32-4fc9-8790-e251107208b7
2018-09-23 12:21:57,653 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-23fee418-979e-4c1b-8141-e0458dcd4805
2018-09-23 12:21:57,653 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-ubuntu/dfs/data/current, StorageType: DISK
2018-09-23 12:21:57,662 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-09-23 12:21:57,662 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-880181939-172.31.87.173-1537705260947
2018-09-23 12:21:57,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-880181939-172.31.87.173-1537705260947 on volume /tmp/hadoop-ubuntu/dfs/data/current...
2018-09-23 12:21:57,689 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-880181939-172.31.87.173-1537705260947 on /tmp/hadoop-ubuntu/dfs/data/current: 20ms
2018-09-23 12:21:57,689 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-880181939-172.31.87.173-1537705260947: 26ms
2018-09-23 12:21:57,690 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-880181939-172.31.87.173-1537705260947 on volume /tmp/hadoop-ubuntu/dfs/data/current...
2018-09-23 12:21:57,690 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-880181939-172.31.87.173-1537705260947 on volume /tmp/hadoop-ubuntu/dfs/data/current: 0ms
2018-09-23 12:21:57,690 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2018-09-23 12:21:57,692 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-880181939-172.31.87.173-1537705260947 on volume /tmp/hadoop-ubuntu/dfs/data
2018-09-23 12:21:57,693 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-ubuntu/dfs/data, DS-23fee418-979e-4c1b-8141-e0458dcd4805): finished scanning block pool BP-880181939-172.31.87.173-1537705260947
2018-09-23 12:21:57,698 ERROR org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: dfs.datanode.directoryscan.throttle.limit.ms.per.sec set to value below 1 ms/sec. Assuming default value of 1000
2018-09-23 12:21:57,698 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1537714074698ms with interval of 21600000ms
2018-09-23 12:21:57,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-880181939-172.31.87.173-1537705260947 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2018-09-23 12:21:57,771 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-ubuntu/dfs/data, DS-23fee418-979e-4c1b-8141-e0458dcd4805): no suitable block pools found to scan.  Waiting 1814399920 ms.
2018-09-23 12:21:57,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-880181939-172.31.87.173-1537705260947 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2018-09-23 12:21:57,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-09-23 12:21:57,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-880181939-172.31.87.173-1537705260947 (Datanode Uuid f9c2d03b-4c32-4fc9-8790-e251107208b7) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2018-09-23 12:21:57,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-880181939-172.31.87.173-1537705260947 (Datanode Uuid f9c2d03b-4c32-4fc9-8790-e251107208b7) service to localhost/127.0.0.1:9000
2018-09-23 12:21:58,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc19d0732914,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 2 msec to generate and 134 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-23 12:21:58,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-23 12:39:06,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd095ec34b34,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-23 12:39:06,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-23 12:56:15,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-880181939-172.31.87.173-1537705260947:blk_1073741825_1001 src: /127.0.0.1:41988 dest: /127.0.0.1:50010
2018-09-23 12:56:15,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41988, dest: /127.0.0.1:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1597370355_1, offset: 0, srvID: f9c2d03b-4c32-4fc9-8790-e251107208b7, blockid: BP-880181939-172.31.87.173-1537705260947:blk_1073741825_1001, duration: 123720487
2018-09-23 12:56:15,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-880181939-172.31.87.173-1537705260947:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 12:56:18,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-880181939-172.31.87.173-1537705260947:blk_1073741826_1002 src: /127.0.0.1:41992 dest: /127.0.0.1:50010
2018-09-23 12:56:18,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41992, dest: /127.0.0.1:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1597370355_1, offset: 0, srvID: f9c2d03b-4c32-4fc9-8790-e251107208b7, blockid: BP-880181939-172.31.87.173-1537705260947:blk_1073741826_1002, duration: 813928
2018-09-23 12:56:18,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-880181939-172.31.87.173-1537705260947:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 12:56:18,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-880181939-172.31.87.173-1537705260947:blk_1073741827_1003 src: /127.0.0.1:41994 dest: /127.0.0.1:50010
2018-09-23 12:56:18,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41994, dest: /127.0.0.1:50010, bytes: 97, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1597370355_1, offset: 0, srvID: f9c2d03b-4c32-4fc9-8790-e251107208b7, blockid: BP-880181939-172.31.87.173-1537705260947:blk_1073741827_1003, duration: 1278492
2018-09-23 12:56:18,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-880181939-172.31.87.173-1537705260947:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 12:56:21,926 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /tmp/hadoop-ubuntu/dfs/data/current/BP-880181939-172.31.87.173-1537705260947/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2018-09-23 12:56:21,928 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /tmp/hadoop-ubuntu/dfs/data/current/BP-880181939-172.31.87.173-1537705260947/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2018-09-23 12:56:21,929 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /tmp/hadoop-ubuntu/dfs/data/current/BP-880181939-172.31.87.173-1537705260947/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2018-09-23 12:56:21,930 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-880181939-172.31.87.173-1537705260947 blk_1073741825_1001 file /tmp/hadoop-ubuntu/dfs/data/current/BP-880181939-172.31.87.173-1537705260947/current/finalized/subdir0/subdir0/blk_1073741825
2018-09-23 12:56:21,930 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-880181939-172.31.87.173-1537705260947 blk_1073741826_1002 file /tmp/hadoop-ubuntu/dfs/data/current/BP-880181939-172.31.87.173-1537705260947/current/finalized/subdir0/subdir0/blk_1073741826
2018-09-23 12:56:21,930 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-880181939-172.31.87.173-1537705260947 blk_1073741827_1003 file /tmp/hadoop-ubuntu/dfs/data/current/BP-880181939-172.31.87.173-1537705260947/current/finalized/subdir0/subdir0/blk_1073741827
2018-09-23 13:30:07,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-880181939-172.31.87.173-1537705260947:blk_1073741828_1004 src: /127.0.0.1:42114 dest: /127.0.0.1:50010
2018-09-23 13:30:07,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42114, dest: /127.0.0.1:50010, bytes: 86424, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_523652930_1, offset: 0, srvID: f9c2d03b-4c32-4fc9-8790-e251107208b7, blockid: BP-880181939-172.31.87.173-1537705260947:blk_1073741828_1004, duration: 69640653
2018-09-23 13:30:07,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-880181939-172.31.87.173-1537705260947:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 13:30:07,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-880181939-172.31.87.173-1537705260947:blk_1073741829_1005 src: /127.0.0.1:42116 dest: /127.0.0.1:50010
2018-09-23 13:30:07,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42116, dest: /127.0.0.1:50010, bytes: 14978, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_523652930_1, offset: 0, srvID: f9c2d03b-4c32-4fc9-8790-e251107208b7, blockid: BP-880181939-172.31.87.173-1537705260947:blk_1073741829_1005, duration: 1085593
2018-09-23 13:30:07,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-880181939-172.31.87.173-1537705260947:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 13:30:07,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-880181939-172.31.87.173-1537705260947:blk_1073741830_1006 src: /127.0.0.1:42118 dest: /127.0.0.1:50010
2018-09-23 13:30:07,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42118, dest: /127.0.0.1:50010, bytes: 1366, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_523652930_1, offset: 0, srvID: f9c2d03b-4c32-4fc9-8790-e251107208b7, blockid: BP-880181939-172.31.87.173-1537705260947:blk_1073741830_1006, duration: 1345838
2018-09-23 13:30:07,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-880181939-172.31.87.173-1537705260947:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 13:32:35,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-880181939-172.31.87.173-1537705260947:blk_1073741831_1007 src: /127.0.0.1:42134 dest: /127.0.0.1:50010
2018-09-23 13:32:35,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42134, dest: /127.0.0.1:50010, bytes: 8, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1206150856_1, offset: 0, srvID: f9c2d03b-4c32-4fc9-8790-e251107208b7, blockid: BP-880181939-172.31.87.173-1537705260947:blk_1073741831_1007, duration: 67616714
2018-09-23 13:32:35,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-880181939-172.31.87.173-1537705260947:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 13:33:05,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-880181939-172.31.87.173-1537705260947:blk_1073741832_1008 src: /127.0.0.1:42140 dest: /127.0.0.1:50010
2018-09-23 13:33:05,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42140, dest: /127.0.0.1:50010, bytes: 16, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_653464340_1, offset: 0, srvID: f9c2d03b-4c32-4fc9-8790-e251107208b7, blockid: BP-880181939-172.31.87.173-1537705260947:blk_1073741832_1008, duration: 48765001
2018-09-23 13:33:05,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-880181939-172.31.87.173-1537705260947:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-23 14:47:54,709 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-23 18:39:08,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x20aec92bffb1,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 1 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-23 18:39:08,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-23 20:47:54,701 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-24 00:39:06,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3453800c1a61,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-24 00:39:06,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-24 02:47:54,701 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-24 06:39:07,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x47f8e8bbde6b,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-24 06:39:07,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-24 08:47:54,701 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-24 12:39:05,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5b9d9eba861b,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 1 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-24 12:39:05,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-24 14:47:54,701 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-24 18:39:06,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6f43098359bf,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-24 18:39:06,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-24 20:47:54,701 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-25 00:39:07,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x82e875afdf52,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-25 00:39:07,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-25 02:47:54,701 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-25 06:39:06,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x968d2c3e174d,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-25 06:39:06,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-25 08:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-25 12:39:07,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xaa3297e3764a,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-25 12:39:07,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-25 14:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-25 18:39:05,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xbdd74e7f8ef1,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-25 18:39:05,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-25 20:47:54,701 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-26 00:39:06,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd17cb53f1d4e,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-26 00:39:06,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-26 02:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-26 06:39:07,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe5221cc41dfa,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-26 06:39:07,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-26 08:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-26 12:39:05,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xf8c6d12b4acb,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-26 12:39:05,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-26 14:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-26 18:39:06,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x10c6c3813af16,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-26 18:39:06,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-26 20:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-27 00:39:07,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x120119fb8fc30,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-27 00:39:07,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-27 02:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-27 06:39:06,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x133b654f2c979,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-27 06:39:06,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-27 08:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-27 12:39:07,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1475bbb6fe344,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-27 12:39:07,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-27 14:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-27 18:39:05,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x15b006f72403a,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-27 18:39:05,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-27 20:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-28 00:39:06,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x16ea5e99c3e1a,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-28 00:39:06,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-28 02:47:54,701 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-28 06:39:07,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1824b50b02f8d,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-28 06:39:07,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-28 08:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-28 12:39:06,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x195f006420803,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-28 12:39:06,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-28 14:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-28 18:39:07,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1a9956f7ae0e6,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-28 18:39:07,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-28 20:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-29 00:39:05,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1bd3a2459c2d5,  containing 1 storage report(s), of which we sent 1. The reports had 5 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-29 00:39:05,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-29 02:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 5, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-29 04:11:09,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-880181939-172.31.87.173-1537705260947:blk_1073741833_1009 src: /127.0.0.1:58696 dest: /127.0.0.1:50010
2018-09-29 04:11:09,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58696, dest: /127.0.0.1:50010, bytes: 48, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1963456428_1, offset: 0, srvID: f9c2d03b-4c32-4fc9-8790-e251107208b7, blockid: BP-880181939-172.31.87.173-1537705260947:blk_1073741833_1009, duration: 58364422
2018-09-29 04:11:09,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-880181939-172.31.87.173-1537705260947:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-09-29 06:39:06,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1d0df8a544148,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-29 06:39:06,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-29 08:47:54,701 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-29 12:39:07,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1e484f130a882,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 1 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-29 12:39:07,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-29 14:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-29 18:39:05,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1f829a6d63db2,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-29 18:39:05,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-29 20:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-30 00:39:06,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x20bcf0f771717,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 1 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-30 00:39:06,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-30 02:47:54,716 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-30 06:39:07,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x21f747746cee3,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-30 06:39:07,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-30 08:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-30 12:39:06,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2331929f023c0,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-30 12:39:06,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-30 14:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-09-30 18:39:07,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x246be937fa074,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-09-30 18:39:07,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-09-30 20:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-01 00:39:05,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x25a6349e3d2d2,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-01 00:39:05,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-01 02:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-01 06:39:06,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x26e08b2454013,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-01 06:39:06,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-01 08:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-01 12:39:07,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x281ae175c6454,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-01 12:39:07,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-01 14:47:54,703 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-01 18:39:05,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x29552ca2a0dd6,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 1 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-01 18:39:05,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-01 20:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-02 00:39:06,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2a8f832141470,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-02 00:39:06,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-02 02:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-02 06:39:07,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2bc9d997bfbb8,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-02 06:39:07,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-02 08:47:54,702 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-02 12:39:06,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2d0424d69b698,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-02 12:39:06,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-02 14:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-02 18:39:07,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2e3e7b420239b,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-02 18:39:07,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-02 20:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-03 00:39:05,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2f78c6978acf3,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-03 00:39:05,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-03 02:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-03 06:39:06,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x30b31cfa9bb58,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-03 06:39:06,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-03 08:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-03 12:39:07,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x31ed734c928ef,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-03 12:39:07,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-03 14:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-03 18:39:05,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3327be90b9c6c,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-03 18:39:05,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-03 20:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-04 00:39:06,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3462150b94e9b,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-04 00:39:06,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-04 02:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-04 06:39:07,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x359c6b687c735,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-04 06:39:07,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-04 08:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-04 12:39:05,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x36d6b69b05189,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-04 12:39:05,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-04 14:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-04 18:39:07,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x38110cee79dec,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-04 18:39:07,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-04 20:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-05 00:39:08,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x394b633a9f5a6,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-05 00:39:08,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-05 02:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-05 06:39:06,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3a85ae9b3f14a,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 8 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-05 06:39:06,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-05 08:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-05 12:39:07,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3bc0050f44c0b,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-05 12:39:07,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-05 14:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-05 18:39:05,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3cfa504a34f7b,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-05 18:39:05,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-05 20:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-06 00:39:06,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3e34a6c8d98e7,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-06 00:39:06,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-06 02:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-06 06:39:07,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3f6efd32786c3,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-06 06:39:07,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-06 08:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-06 12:39:05,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x40a948791a628,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-06 12:39:05,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-06 14:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-06 18:39:07,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x41e39ee45f237,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-06 18:39:07,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-06 20:47:54,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-07 00:39:08,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x431df558a82ce,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-07 00:39:08,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-07 02:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-07 06:39:06,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4458408ff3bbb,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 1 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-07 06:39:06,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-07 08:47:54,702 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-07 12:39:07,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x45929709c53a8,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-07 12:39:07,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-07 14:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-07 18:39:05,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x46cce266950ab,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-07 18:39:05,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-07 20:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-08 00:39:06,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x480738d52c359,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-08 00:39:06,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-08 02:47:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-880181939-172.31.87.173-1537705260947 Total blocks: 6, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-08 06:39:07,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x49418f35acff6,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-08 06:39:07,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-880181939-172.31.87.173-1537705260947
2018-10-08 07:30:51,772 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-10-08 07:30:51,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-10-14 11:56:31,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-10-14 11:56:31,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-10-14 11:56:32,390 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-10-14 11:56:32,451 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-10-14 11:56:32,451 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-10-14 11:56:32,457 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-10-14 11:56:32,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-10-14 11:56:32,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-10-14 11:56:32,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-10-14 11:56:32,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-10-14 11:56:32,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-10-14 11:56:32,570 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-10-14 11:56:32,579 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-10-14 11:56:32,597 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-10-14 11:56:32,602 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-10-14 11:56:32,604 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-10-14 11:56:32,604 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-10-14 11:56:32,605 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-10-14 11:56:32,618 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 36047
2018-10-14 11:56:32,618 INFO org.mortbay.log: jetty-6.1.26
2018-10-14 11:56:32,760 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36047
2018-10-14 11:56:32,881 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-10-14 11:56:33,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-10-14 11:56:33,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-10-14 11:56:33,144 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-10-14 11:56:33,159 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-10-14 11:56:33,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-10-14 11:56:33,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-10-14 11:56:33,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-10-14 11:56:33,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-10-14 11:56:33,281 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-10-14 11:56:33,290 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-10-14 11:56:34,349 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:35,350 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:36,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:37,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:38,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:39,353 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:40,354 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:41,354 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:42,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:43,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:43,358 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 11:56:49,359 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:50,360 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:51,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:52,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:53,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:54,363 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:55,364 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:56,364 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:57,365 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:58,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:56:58,369 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 11:57:04,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:05,371 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:06,371 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:07,372 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:08,373 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:09,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:10,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:11,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:12,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:13,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:13,378 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 11:57:19,380 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:20,380 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:21,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:22,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:23,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:24,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:25,384 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:26,385 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:27,385 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:28,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:28,389 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 11:57:34,390 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:35,391 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:36,391 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:37,392 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:38,393 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:39,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:40,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:41,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:42,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:43,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:43,400 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 11:57:49,401 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:50,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:51,403 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:52,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:53,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:54,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:55,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:56,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:57,407 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:58,408 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:57:58,411 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 11:58:04,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:05,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:06,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:07,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:08,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:09,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:10,416 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:11,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:12,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:13,418 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:13,421 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 11:58:19,422 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:20,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:21,424 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:22,424 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:23,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:24,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:25,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:26,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:27,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:28,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:28,430 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 11:58:34,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:35,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:36,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:37,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:38,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:39,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:40,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:41,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:42,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:43,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:43,440 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 11:58:49,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:50,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:51,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:52,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:53,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:54,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:55,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:56,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:57,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:58,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:58:58,449 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 11:59:04,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:59:05,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:59:06,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:59:07,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:59:08,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:59:09,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:59:10,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:59:11,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:59:12,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:59:13,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:59:13,458 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 11:59:19,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:59:20,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:59:21,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:59:22,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 11:59:22,976 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-10-14 11:59:22,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-10-14 11:59:57,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-10-14 11:59:57,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-10-14 11:59:57,715 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-10-14 11:59:57,773 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-10-14 11:59:57,773 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-10-14 11:59:57,777 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-10-14 11:59:57,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-10-14 11:59:57,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-10-14 11:59:57,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-10-14 11:59:57,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-10-14 11:59:57,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-10-14 11:59:57,877 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-10-14 11:59:57,885 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-10-14 11:59:57,897 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-10-14 11:59:57,902 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-10-14 11:59:57,904 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-10-14 11:59:57,904 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-10-14 11:59:57,904 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-10-14 11:59:57,914 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 39337
2018-10-14 11:59:57,914 INFO org.mortbay.log: jetty-6.1.26
2018-10-14 11:59:58,045 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:39337
2018-10-14 11:59:58,126 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-10-14 11:59:58,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-10-14 11:59:58,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-10-14 11:59:58,389 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-10-14 11:59:58,402 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-10-14 11:59:58,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-10-14 11:59:58,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-10-14 11:59:58,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-10-14 11:59:58,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-10-14 11:59:58,503 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-10-14 11:59:58,504 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-10-14 11:59:59,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:00,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:01,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:02,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:03,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:04,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:05,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:06,572 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:07,573 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:08,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:08,578 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 12:00:14,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:15,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:16,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:17,582 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:18,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:19,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:20,584 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:21,047 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-10-14 12:00:21,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ip-172-31-87-173.ec2.internal/172.31.87.173
************************************************************/
2018-10-14 12:00:56,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ip-172-31-87-173.ec2.internal/172.31.87.173
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.6
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-2.7.6/etc/hadoop:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/common/hadoop-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/home/ubuntu/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by 'kshvachk' on 2018-04-18T01:33Z
STARTUP_MSG:   java = 1.8.0_181
************************************************************/
2018-10-14 12:00:56,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-10-14 12:00:56,967 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-10-14 12:00:57,033 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2018-10-14 12:00:57,033 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-10-14 12:00:57,037 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-10-14 12:00:57,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ip-172-31-87-173.ec2.internal
2018-10-14 12:00:57,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-10-14 12:00:57,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-10-14 12:00:57,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2018-10-14 12:00:57,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2018-10-14 12:00:57,147 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-10-14 12:00:57,155 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-10-14 12:00:57,169 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-10-14 12:00:57,174 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-10-14 12:00:57,175 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-10-14 12:00:57,175 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-10-14 12:00:57,175 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-10-14 12:00:57,186 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 32933
2018-10-14 12:00:57,186 INFO org.mortbay.log: jetty-6.1.26
2018-10-14 12:00:57,310 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:32933
2018-10-14 12:00:57,397 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-10-14 12:00:57,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = ubuntu
2018-10-14 12:00:57,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-10-14 12:00:57,671 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2018-10-14 12:00:57,689 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-10-14 12:00:57,742 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-10-14 12:00:57,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-10-14 12:00:57,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-10-14 12:00:57,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2018-10-14 12:00:57,809 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-10-14 12:00:57,811 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-10-14 12:00:58,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:00:59,874 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:00,874 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:01,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:02,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:03,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:04,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:05,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:06,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:07,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:07,882 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 12:01:13,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:14,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:15,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:16,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:17,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:18,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:19,887 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:20,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:21,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:22,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:22,892 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 12:01:28,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:29,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:30,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:31,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:32,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:33,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:34,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:35,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:36,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:37,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:37,901 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 12:01:43,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:44,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:45,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:46,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:47,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:48,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:49,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:50,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:51,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:52,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:52,912 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 12:01:58,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:01:59,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:00,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:01,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:02,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:03,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:04,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:05,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:06,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:07,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:07,922 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 12:02:13,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:14,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:15,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:16,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:17,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:18,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:19,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:20,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:21,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:22,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:22,931 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 12:02:28,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:29,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:30,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:31,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:32,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:33,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:34,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:35,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:36,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:37,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:37,941 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 12:02:43,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:44,943 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:45,943 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:46,944 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:47,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:48,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:49,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:50,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:51,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:52,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:52,950 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 12:02:58,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:02:59,952 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:03:00,952 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:03:01,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:03:02,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:03:03,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:03:04,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:03:05,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:03:06,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:03:07,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:03:07,960 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2018-10-14 12:03:13,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-10-14 12:03:14,134 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2018-10-14 12:03:14,141 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/ubuntu/hadoop_data/datanode/in_use.lock acquired by nodename 18589@ip-172-31-87-173.ec2.internal
2018-10-14 12:03:14,142 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/ubuntu/hadoop_data/datanode is not formatted for namespace 1383172426. Formatting...
2018-10-14 12:03:14,143 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-d98edc85-b913-4592-a4b5-41395da0e264 for directory /home/ubuntu/hadoop_data/datanode
2018-10-14 12:03:14,288 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1256777296-172.31.87.173-1539518579601
2018-10-14 12:03:14,288 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/ubuntu/hadoop_data/datanode/current/BP-1256777296-172.31.87.173-1539518579601
2018-10-14 12:03:14,288 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /home/ubuntu/hadoop_data/datanode/current/BP-1256777296-172.31.87.173-1539518579601 is not formatted for BP-1256777296-172.31.87.173-1539518579601. Formatting ...
2018-10-14 12:03:14,288 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1256777296-172.31.87.173-1539518579601 directory /home/ubuntu/hadoop_data/datanode/current/BP-1256777296-172.31.87.173-1539518579601/current
2018-10-14 12:03:14,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1383172426;bpid=BP-1256777296-172.31.87.173-1539518579601;lv=-56;nsInfo=lv=-63;cid=CID-aec84a3f-9abe-4c10-aacd-389fb3905504;nsid=1383172426;c=0;bpid=BP-1256777296-172.31.87.173-1539518579601;dnuuid=null
2018-10-14 12:03:14,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 82e09beb-65b0-4d15-8d85-eefe7df4e472
2018-10-14 12:03:14,396 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-d98edc85-b913-4592-a4b5-41395da0e264
2018-10-14 12:03:14,396 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/ubuntu/hadoop_data/datanode/current, StorageType: DISK
2018-10-14 12:03:14,411 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-10-14 12:03:14,411 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-14 12:03:14,413 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1256777296-172.31.87.173-1539518579601 on volume /home/ubuntu/hadoop_data/datanode/current...
2018-10-14 12:03:14,446 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1256777296-172.31.87.173-1539518579601 on /home/ubuntu/hadoop_data/datanode/current: 28ms
2018-10-14 12:03:14,446 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1256777296-172.31.87.173-1539518579601: 35ms
2018-10-14 12:03:14,453 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1256777296-172.31.87.173-1539518579601 on volume /home/ubuntu/hadoop_data/datanode/current...
2018-10-14 12:03:14,454 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1256777296-172.31.87.173-1539518579601 on volume /home/ubuntu/hadoop_data/datanode/current: 0ms
2018-10-14 12:03:14,454 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 8ms
2018-10-14 12:03:14,457 ERROR org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: dfs.datanode.directoryscan.throttle.limit.ms.per.sec set to value below 1 ms/sec. Assuming default value of 1000
2018-10-14 12:03:14,457 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1256777296-172.31.87.173-1539518579601 on volume /home/ubuntu/hadoop_data/datanode
2018-10-14 12:03:14,458 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/ubuntu/hadoop_data/datanode, DS-d98edc85-b913-4592-a4b5-41395da0e264): finished scanning block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-14 12:03:14,462 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1539520802462ms with interval of 21600000ms
2018-10-14 12:03:14,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1256777296-172.31.87.173-1539518579601 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2018-10-14 12:03:14,538 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/ubuntu/hadoop_data/datanode, DS-d98edc85-b913-4592-a4b5-41395da0e264): no suitable block pools found to scan.  Waiting 1814399918 ms.
2018-10-14 12:03:14,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1256777296-172.31.87.173-1539518579601 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2018-10-14 12:03:14,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-10-14 12:03:14,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1256777296-172.31.87.173-1539518579601 (Datanode Uuid 82e09beb-65b0-4d15-8d85-eefe7df4e472) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=3
2018-10-14 12:03:14,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1256777296-172.31.87.173-1539518579601 (Datanode Uuid 82e09beb-65b0-4d15-8d85-eefe7df4e472) service to localhost/127.0.0.1:9000
2018-10-14 12:03:14,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1e63275be9b2a,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 2 msec to generate and 73 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-14 12:03:14,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-14 12:12:26,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1256777296-172.31.87.173-1539518579601:blk_1073741825_1001 src: /127.0.0.1:38938 dest: /127.0.0.1:50010
2018-10-14 12:12:26,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38938, dest: /127.0.0.1:50010, bytes: 686, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1787774162_1, offset: 0, srvID: 82e09beb-65b0-4d15-8d85-eefe7df4e472, blockid: BP-1256777296-172.31.87.173-1539518579601:blk_1073741825_1001, duration: 41996804
2018-10-14 12:12:26,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1256777296-172.31.87.173-1539518579601:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-10-14 12:40:02,467 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 1, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-14 13:06:12,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1256777296-172.31.87.173-1539518579601:blk_1073741826_1002 src: /127.0.0.1:39094 dest: /127.0.0.1:50010
2018-10-14 13:06:12,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39094, dest: /127.0.0.1:50010, bytes: 685, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_54348197_1, offset: 0, srvID: 82e09beb-65b0-4d15-8d85-eefe7df4e472, blockid: BP-1256777296-172.31.87.173-1539518579601:blk_1073741826_1002, duration: 27947269
2018-10-14 13:06:12,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1256777296-172.31.87.173-1539518579601:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-10-14 13:06:14,855 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /home/ubuntu/hadoop_data/datanode/current/BP-1256777296-172.31.87.173-1539518579601/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2018-10-14 13:06:14,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1256777296-172.31.87.173-1539518579601 blk_1073741825_1001 file /home/ubuntu/hadoop_data/datanode/current/BP-1256777296-172.31.87.173-1539518579601/current/finalized/subdir0/subdir0/blk_1073741825
2018-10-14 17:44:27,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1f8d135ea3780,  containing 1 storage report(s), of which we sent 1. The reports had 1 total blocks and used 1 RPC(s). This took 1 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-14 17:44:27,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-14 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 1, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-14 23:44:25,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x20c75eabacc35,  containing 1 storage report(s), of which we sent 1. The reports had 1 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-14 23:44:25,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-15 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 1, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-15 05:44:26,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2201b50d68304,  containing 1 storage report(s), of which we sent 1. The reports had 1 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-15 05:44:26,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-15 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 1, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-15 07:20:30,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1256777296-172.31.87.173-1539518579601:blk_1073741827_1003 src: /127.0.0.1:41412 dest: /127.0.0.1:50010
2018-10-15 07:20:30,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41412, dest: /127.0.0.1:50010, bytes: 286, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1060019114_1, offset: 0, srvID: 82e09beb-65b0-4d15-8d85-eefe7df4e472, blockid: BP-1256777296-172.31.87.173-1539518579601:blk_1073741827_1003, duration: 23549012
2018-10-15 07:20:30,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1256777296-172.31.87.173-1539518579601:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-10-15 11:44:28,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x233c0b7078aa8,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-15 11:44:28,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-15 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-15 17:44:26,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x247656b38967d,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-15 17:44:26,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-15 18:40:02,465 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-15 23:44:27,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x25b0ad1b92a5e,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-15 23:44:27,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-16 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-16 05:44:25,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x26eaf8986ef4f,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-16 05:44:25,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-16 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-16 11:44:26,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x28254f29324d5,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-16 11:44:26,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-16 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-16 17:44:27,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x295fa5891da05,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-16 17:44:27,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-16 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-16 23:44:25,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2a99f0a2bfa4b,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-16 23:44:25,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-17 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-17 05:44:26,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2bd446d6b1a93,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-17 05:44:26,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-17 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-17 11:44:27,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2d0e9d07219cb,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-17 11:44:27,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-17 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-17 17:44:25,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2e48e8029dad4,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-17 17:44:25,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-17 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-17 23:44:27,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2f833e4388138,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-17 23:44:27,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-18 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-18 05:44:28,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x30bd947c9e5f7,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-18 05:44:28,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-18 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-18 11:44:26,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x31f7dfa6efde1,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-18 11:44:26,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-18 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-18 17:44:27,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x333235e393df8,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-18 17:44:27,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-18 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-18 23:44:28,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x346c8c1219d40,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-18 23:44:28,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-19 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-19 05:44:26,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x35a6d75216c94,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-19 05:44:26,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-19 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-19 11:44:27,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x36e12da0649ed,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-19 11:44:27,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-19 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-19 17:44:25,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x381b78b4d60e4,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-19 17:44:25,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-19 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-19 23:44:26,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3955ceee632e4,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-19 23:44:26,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-20 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-20 05:44:27,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3a90251a9cac3,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-20 05:44:27,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-20 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-20 11:44:25,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3bca70500a1c0,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-20 11:44:25,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-20 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-20 17:44:26,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3d04c66d0adc2,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-20 17:44:26,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-20 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-20 23:44:27,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3e3f1ca0021e1,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-20 23:44:27,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-21 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-21 05:44:25,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3f7967a523da7,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-21 05:44:25,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-21 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-21 11:44:27,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x40b3bdebfb0eb,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-21 11:44:27,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-21 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-21 17:44:28,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x41ee141ed917f,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-21 17:44:28,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-21 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-21 23:44:26,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x43285f2e2860b,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-21 23:44:26,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-22 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-22 05:44:27,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4462b569e3f22,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-22 05:44:27,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-22 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-22 11:44:28,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x459d0bb06c947,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-22 11:44:28,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-22 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-22 17:44:26,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x46d756e1a9c0b,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-22 17:44:26,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-22 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-22 23:44:27,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4811ad47e4281,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-22 23:44:27,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-23 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-23 05:44:25,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x494bf850a4d2a,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-23 05:44:25,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-23 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-23 11:44:26,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4a864e7fa787a,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-23 11:44:26,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-23 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-23 17:44:27,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4bc0a4a7357a9,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-23 17:44:27,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-23 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-23 23:44:25,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4cfaefb0a823e,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-23 23:44:25,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-24 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-24 05:44:26,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4e35460be3048,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-24 05:44:26,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-24 06:40:02,464 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-24 11:44:27,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4f6f9c3d46cd3,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-24 11:44:27,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-24 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-24 17:44:25,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x50a9e741b16d3,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-24 17:44:25,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-24 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-24 23:44:26,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x51e43d7868181,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-24 23:44:26,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-25 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-25 05:44:28,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x531e93c1a4685,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-25 05:44:28,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-25 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-25 11:44:26,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5458dedfe7358,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-25 11:44:26,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-25 12:40:02,464 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-25 17:44:27,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5593351fcdaed,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-25 17:44:27,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-25 18:40:02,464 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-25 23:44:28,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x56cd8b5af8ab7,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-25 23:44:28,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-26 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-26 05:44:26,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5807d663f5450,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-26 05:44:26,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-26 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-26 11:44:27,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x59422cc0271ce,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-26 11:44:27,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-26 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-26 17:44:28,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5a7c83049a928,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-26 17:44:28,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-26 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-26 23:44:26,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5bb6ce1545209,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-26 23:44:26,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-27 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-27 05:44:27,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5cf1245eee11f,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-27 05:44:27,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-27 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-27 11:44:25,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5e2b6f6237b80,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-27 11:44:25,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-27 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-27 17:44:26,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5f65c5d1a02df,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-27 17:44:26,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-27 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-27 23:44:27,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x60a01c1c67986,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-27 23:44:27,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-28 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-28 05:44:25,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x61da672bd099e,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-28 05:44:25,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-28 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-28 11:44:26,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6314bd687a7b8,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-28 11:44:26,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-28 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-28 17:44:28,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x644f139db2c09,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-28 17:44:28,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-28 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-28 23:44:26,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x65895ea2e6955,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-28 23:44:26,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-29 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-29 05:44:27,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x66c3b4d64ee9d,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-29 05:44:27,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-29 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-29 11:44:28,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x67fe0b1f7f719,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-29 11:44:28,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-29 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-29 17:44:26,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6938562d5e684,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-29 17:44:26,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-29 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-29 23:44:27,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6a72ac7dcf16d,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-29 23:44:27,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-30 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-30 05:44:28,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6bad02c252905,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-30 05:44:28,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-30 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-30 11:44:26,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6ce74dcf1a164,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-30 11:44:26,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-30 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-30 17:44:27,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6e21a412c2e11,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-30 17:44:27,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-30 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-30 23:44:25,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6f5bef31e0cfd,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-30 23:44:25,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-31 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-31 05:44:26,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x709645783b1b4,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-31 05:44:26,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-31 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-31 11:44:27,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x71d09bc4ef23f,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-31 11:44:27,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-31 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-31 17:44:25,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x730ae6ccfe25c,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-31 17:44:25,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-10-31 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-10-31 23:44:26,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x74453d28b03b4,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-10-31 23:44:26,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-01 00:40:02,465 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-01 05:44:28,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x757f9381436eb,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-01 05:44:28,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-01 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-01 11:44:26,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x76b9de9fe3d01,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-01 11:44:26,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-01 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-01 17:44:27,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x77f434f0a3491,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-01 17:44:27,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-01 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-01 23:44:28,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x792e8b3084282,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-01 23:44:28,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-02 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-02 05:44:26,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x7a68d648a8c1b,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-02 05:44:26,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-02 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-02 11:44:27,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x7ba32c98df487,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-02 11:44:27,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-02 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-02 17:44:25,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x7cdd779bb10b8,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-02 17:44:25,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-02 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-02 23:44:26,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x7e17cdfd8cff1,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-02 23:44:26,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-03 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-03 05:44:27,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x7f52244f4d059,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-03 05:44:27,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-03 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-03 11:44:25,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x808c6f640c72f,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-03 11:44:25,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-03 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-03 17:44:26,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x81c6c5a611226,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-03 17:44:26,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-03 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-03 23:44:27,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x83011bef08b85,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-03 23:44:27,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-04 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-04 05:44:25,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x843b66fadbba4,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-04 05:44:25,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-04 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-04 11:44:27,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8575bd47668f8,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-04 11:44:27,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-04 12:03:14,467 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now rescanning bpid BP-1256777296-172.31.87.173-1539518579601 on volume /home/ubuntu/hadoop_data/datanode, after more than 504 hour(s)
2018-11-04 12:03:14,470 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/ubuntu/hadoop_data/datanode, DS-d98edc85-b913-4592-a4b5-41395da0e264): finished scanning block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-04 12:03:14,471 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/ubuntu/hadoop_data/datanode, DS-d98edc85-b913-4592-a4b5-41395da0e264): no suitable block pools found to scan.  Waiting 1814399996 ms.
2018-11-04 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-04 17:44:28,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x86b013846bcf9,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-04 17:44:28,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-04 18:40:02,465 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-04 23:44:26,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x87ea5e8c48fc4,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-04 23:44:26,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-05 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-05 05:44:27,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8924b4b5a8f42,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-05 05:44:27,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-05 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-05 11:44:28,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8a5f0af9f1c47,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-05 11:44:28,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-05 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-05 17:44:26,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8b995614e4908,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-05 17:44:26,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-05 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-05 23:44:27,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8cd3ac5ccf280,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-05 23:44:27,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-06 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-06 05:44:25,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8e0df762d067c,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-06 05:44:25,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-06 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-06 11:44:26,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8f484d9a6441b,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-06 11:44:26,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-06 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-06 17:44:27,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9082a3ea1a3e8,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-06 17:44:27,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-06 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-06 23:44:25,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x91bcef007520e,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-06 23:44:25,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-07 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-07 05:44:26,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x92f7454626b28,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-07 05:44:26,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-07 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-07 11:44:27,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x94319b7a58172,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-07 11:44:27,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-07 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-07 17:44:25,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x956be697ccbc5,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-07 17:44:25,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-07 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-07 23:44:26,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x96a63ccc9b6c4,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-07 23:44:26,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-08 00:40:02,465 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-08 05:44:28,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x97e092f9c91e6,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-08 05:44:28,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-08 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-08 11:44:26,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x991ade09e8637,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-08 11:44:26,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-08 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-08 17:44:27,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9a55343e164da,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-08 17:44:27,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-08 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-08 23:44:28,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9b8f8a88cfb05,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-08 23:44:28,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-09 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-09 05:44:26,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9cc9d5a707f26,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-09 05:44:26,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-09 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-09 11:44:27,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9e042bd6b2e1c,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-09 11:44:27,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-09 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-09 17:44:28,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9f3e82127ade4,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-09 17:44:28,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-09 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-09 23:44:26,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa078cd2467d28,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-09 23:44:26,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-10 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-10 05:44:27,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa1b32359bec92,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-10 05:44:27,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-10 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-10 11:44:25,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa2ed6e75747bc,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-10 11:44:25,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-10 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-10 17:44:26,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa427c4a15f844,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-10 17:44:26,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-10 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-10 23:44:27,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa5621af12b024,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-10 23:44:27,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-11 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-11 05:44:25,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa69c6602e3986,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-11 05:44:25,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-11 06:40:02,462 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-11 11:44:26,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa7d6bc47a46b6,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-11 11:44:26,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-11 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-11 17:44:27,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa9111269c7fca,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-11 17:44:27,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-11 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-11 23:44:26,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xaa4b5d754e5f5,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-11 23:44:26,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-12 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-12 05:44:27,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xab85b39a81903,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-12 05:44:27,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-12 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-12 11:44:28,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xacc009ced91e1,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-12 11:44:28,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-12 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-12 17:44:26,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xadfa54e3e6401,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-12 17:44:26,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-12 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-12 23:44:27,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xaf34ab162ba27,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-12 23:44:27,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-13 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-13 05:44:28,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xb06f014da5d5e,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-13 05:44:28,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-13 06:40:02,465 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-13 11:44:26,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xb1a94c5ba01a0,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-13 11:44:26,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-13 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-13 17:44:27,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xb2e3a28688c5a,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-13 17:44:27,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-13 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-13 23:44:28,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xb41df8be6fe20,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-13 23:44:28,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-14 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-14 05:44:26,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xb55843c6e38b6,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-14 05:44:26,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-14 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-14 11:44:27,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xb6929a04ad55a,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-14 11:44:27,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-14 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-14 17:44:25,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xb7cce512cfabc,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-14 17:44:25,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-14 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-14 23:44:26,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xb9073b5829299,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-14 23:44:26,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-15 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-15 05:44:27,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xba419197665af,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 5 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-15 05:44:27,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-15 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-15 11:44:25,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xbb7bdca6f4494,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-15 11:44:25,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-15 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-15 17:44:26,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xbcb632f801766,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-15 17:44:26,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-15 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-15 23:44:27,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xbdf0892e8a029,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-15 23:44:27,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-16 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-16 05:44:26,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xbf2ad437f1527,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-16 05:44:26,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-16 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-16 11:44:27,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc0652a5c935f3,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-16 11:44:27,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-16 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-16 17:44:28,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc19f80b2c57c4,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-16 17:44:28,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-16 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-16 23:44:26,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc2d9cbd6795fb,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-16 23:44:26,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-17 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-17 05:44:27,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc4142247eaa83,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-17 05:44:27,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-17 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-17 11:44:25,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc54e6d8dec40e,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-17 11:44:25,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-17 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-17 17:44:26,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc688c3f36784c,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-17 17:44:26,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-17 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-17 23:44:27,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc7c31a50ca45c,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-17 23:44:27,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-18 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-18 05:44:25,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc8fd6586ddceb,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-18 05:44:25,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-18 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-18 11:44:26,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xca37bbfc0bfcb,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-18 11:44:26,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-18 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-18 17:44:28,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xcb72127778b78,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-18 17:44:28,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-18 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-18 23:44:26,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xccac5daf72f75,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-18 23:44:26,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-19 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-19 05:44:27,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xcde6b403de4dd,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-19 05:44:27,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-19 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-19 11:44:28,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xcf210a552032b,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-19 11:44:28,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-19 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-19 17:44:26,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd05b5580f9eda,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-19 17:44:26,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-19 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-19 23:44:27,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd195abf00408b,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-19 23:44:27,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-20 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-20 05:44:25,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd2cff70fbc231,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-20 05:44:25,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-20 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-20 11:44:26,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd40a4d6e37197,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-20 11:44:26,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-20 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-20 17:44:27,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd544a3df891f8,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-20 17:44:27,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-20 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-20 23:44:25,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd67eeef7b4f2f,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-20 23:44:25,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-21 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-21 05:44:27,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd7b9456370f4f,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-21 05:44:27,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-21 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-21 11:44:28,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd8f39bd760c7b,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-21 11:44:28,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-21 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-21 17:44:26,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xda2de6ec01c41,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-21 17:44:26,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-21 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-21 23:44:27,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xdb683d3b657d6,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-21 23:44:27,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-22 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-22 05:44:28,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xdca2939976eea,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-22 05:44:28,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-22 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-22 11:44:26,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xdddcdeb01dfe3,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-22 11:44:26,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-22 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-22 17:44:27,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xdf173528a722a,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-22 17:44:27,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-22 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-22 23:44:25,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe051805599c9f,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-22 23:44:25,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-23 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-23 05:44:26,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe18bd6bc74d82,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-23 05:44:26,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-23 06:40:02,462 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-23 11:44:27,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe2c62d12ba9a1,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-23 11:44:27,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-23 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-23 17:44:26,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe400783423fe6,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-23 17:44:26,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-23 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-23 23:44:27,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe53ace9be812f,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-23 23:44:27,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-24 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-24 05:44:28,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe6752503ba55f,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-24 05:44:28,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-24 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-24 11:44:26,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe7af703748c80,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-24 11:44:26,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-24 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-24 17:44:27,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe8e9c6abbc33d,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-24 17:44:27,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-24 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-24 23:44:25,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xea2411fa27c5c,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-24 23:44:25,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-25 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-25 05:44:26,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xeb5e6864581d8,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-25 05:44:26,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-25 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-25 11:44:27,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xec98bebcdaee9,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-25 11:44:27,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-25 12:03:14,477 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now rescanning bpid BP-1256777296-172.31.87.173-1539518579601 on volume /home/ubuntu/hadoop_data/datanode, after more than 504 hour(s)
2018-11-25 12:03:14,479 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/ubuntu/hadoop_data/datanode, DS-d98edc85-b913-4592-a4b5-41395da0e264): finished scanning block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-25 12:03:14,481 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/ubuntu/hadoop_data/datanode, DS-d98edc85-b913-4592-a4b5-41395da0e264): no suitable block pools found to scan.  Waiting 1814399996 ms.
2018-11-25 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-25 17:44:25,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xedd309ffa5b43,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-25 17:44:25,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-25 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-25 23:44:27,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xef0d607ba87fa,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-25 23:44:27,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-26 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-26 05:44:28,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xf047b70131957,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-26 05:44:28,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-26 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-26 11:44:26,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xf1820224fd6a5,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-26 11:44:26,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-26 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-26 17:44:27,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xf2bc5889d8902,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-26 17:44:27,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-26 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-26 23:44:25,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xf3f6a3b41561c,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-26 23:44:25,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-27 00:40:02,462 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-27 05:44:26,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xf530f9e452ec0,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-27 05:44:26,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-27 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-27 11:44:27,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xf66b5026485a7,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-27 11:44:27,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-27 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-27 17:44:25,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xf7a59b6af3713,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-27 17:44:25,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-27 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-27 23:44:26,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xf8dff1cf0dae5,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-27 23:44:26,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-28 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-28 05:44:27,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xfa1a4802d16f7,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-28 05:44:27,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-28 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-28 11:44:25,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xfb549327a14c9,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-28 11:44:25,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-28 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-28 17:44:27,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xfc8ee96eec299,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-28 17:44:27,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-28 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-28 23:44:28,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xfdc93fb57607c,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-28 23:44:28,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-29 00:40:02,462 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-29 05:44:26,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xff038ac8de552,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-29 05:44:26,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-29 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-29 11:44:27,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1003de0f7a965e,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-29 11:44:27,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-29 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-29 17:44:28,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1017837428fe1a,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-29 17:44:28,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-29 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-29 23:44:26,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x102b2824631c57,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-29 23:44:26,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-30 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-30 05:44:27,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x103ecd887c418a,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-30 05:44:27,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-30 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-30 11:44:25,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x10527238631e03,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-30 11:44:25,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-30 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-30 17:44:26,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1066179df08a9b,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-30 17:44:26,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-11-30 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-11-30 23:44:27,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1079bd01c1a853,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-30 23:44:27,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-01 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-01 05:44:25,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x108d61b581a649,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 5 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-01 05:44:25,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-01 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-01 11:44:26,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x10a1071bd15ffb,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-01 11:44:26,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-01 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-01 17:44:27,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x10b4ac819332a5,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-01 17:44:27,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-01 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-01 23:44:26,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x10c85136442b2a,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-01 23:44:26,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-02 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-02 05:44:27,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x10dbf69decbe26,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-02 05:44:27,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-02 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-02 11:44:28,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x10ef9c031ef428,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-02 11:44:28,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-02 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-02 17:44:26,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x110340b82ef024,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-02 17:44:26,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-02 18:40:02,462 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-02 23:44:27,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1116e61e2294a8,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-02 23:44:27,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-03 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-03 05:44:25,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x112a8acf8ce596,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-03 05:44:25,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-03 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-03 11:44:26,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x113e3034257fdf,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-03 11:44:26,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-03 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-03 17:44:27,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1151d59b076bc7,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-03 17:44:27,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-03 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-03 23:44:25,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x11657a4f86d481,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-03 23:44:25,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-04 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-04 05:44:27,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x11791fb6f70e60,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-04 05:44:27,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-04 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-04 11:44:28,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x118cc51cdb13d8,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-04 11:44:28,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-04 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-04 17:44:26,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x11a069d060863f,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-04 17:44:26,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-04 18:40:02,462 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-04 23:44:27,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x11b40f3756cd02,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-04 23:44:27,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-05 00:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-05 05:44:25,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x11c7b3eb0fbd7b,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-05 05:44:25,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-05 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-05 11:44:26,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x11db59526e8fd3,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-05 11:44:26,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-05 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-05 17:44:27,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x11eefeb80f35a8,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 0 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-05 17:44:27,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-05 18:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-05 23:44:25,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1202a36d2df0d3,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-05 23:44:25,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-06 00:40:02,462 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-06 05:44:26,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x121648d2f878f1,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-06 05:44:26,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-06 06:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-06 11:44:28,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1229ee3b59e1b0,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-12-06 11:44:28,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1256777296-172.31.87.173-1539518579601
2018-12-06 12:40:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1256777296-172.31.87.173-1539518579601 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-12-06 16:11:56,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1256777296-172.31.87.173-1539518579601:blk_1073741828_1004 src: /127.0.0.1:57236 dest: /127.0.0.1:50010
2018-12-06 16:11:56,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57236, dest: /127.0.0.1:50010, bytes: 685, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1416745057_1, offset: 0, srvID: 82e09beb-65b0-4d15-8d85-eefe7df4e472, blockid: BP-1256777296-172.31.87.173-1539518579601:blk_1073741828_1004, duration: 21316812
2018-12-06 16:11:56,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1256777296-172.31.87.173-1539518579601:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2018-12-06 16:12:20,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1256777296-172.31.87.173-1539518579601:blk_1073741829_1005 src: /127.0.0.1:57244 dest: /127.0.0.1:50010
2018-12-06 16:12:20,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57244, dest: /127.0.0.1:50010, bytes: 286, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1456306115_1, offset: 0, srvID: 82e09beb-65b0-4d15-8d85-eefe7df4e472, blockid: BP-1256777296-172.31.87.173-1539518579601:blk_1073741829_1005, duration: 22097158
2018-12-06 16:12:20,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1256777296-172.31.87.173-1539518579601:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
